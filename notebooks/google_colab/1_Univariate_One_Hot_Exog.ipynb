{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "afb46504a33b46db8069a824f4084232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48592bf461c547f0ab8468f626c66c54",
              "IPY_MODEL_e36e99aa55c143149bd58bc8de00a002",
              "IPY_MODEL_31e5af6c515a4f3f8a4bdba1b1ecb3da"
            ],
            "layout": "IPY_MODEL_8b7721087c0f43379756427d2ccc3ec9"
          }
        },
        "48592bf461c547f0ab8468f626c66c54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_052b70e0b6f34327bd68474511600f4a",
            "placeholder": "​",
            "style": "IPY_MODEL_42c259d6e7fa4d3e846c257a9ea6e040",
            "value": "Best trial: 0. Best value: 1.11605: 100%"
          }
        },
        "e36e99aa55c143149bd58bc8de00a002": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd2d6054570b4b188894d243c5cbb3b7",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7ae9c9bf2caa4a4db82af713a93e42c4",
            "value": 3
          }
        },
        "31e5af6c515a4f3f8a4bdba1b1ecb3da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88ea1fb63f684e4d88bfa999762df6aa",
            "placeholder": "​",
            "style": "IPY_MODEL_7169efedb6714a9aae167ea3cdc105b8",
            "value": " 3/3 [03:56&lt;00:00, 76.73s/it, 235.99/900 seconds]"
          }
        },
        "8b7721087c0f43379756427d2ccc3ec9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "052b70e0b6f34327bd68474511600f4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42c259d6e7fa4d3e846c257a9ea6e040": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd2d6054570b4b188894d243c5cbb3b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ae9c9bf2caa4a4db82af713a93e42c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88ea1fb63f684e4d88bfa999762df6aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7169efedb6714a9aae167ea3cdc105b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Automotive Industry Trends Forecasting using Neural Networks\n",
        "\n",
        "> **Supervised and Unsupervised Learning Course | HdM Stuttgart **\n",
        "> Authors: John Torres, Samuel Hempelt\n",
        "\n",
        "[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n",
        "\n",
        "---\n",
        "\n",
        "## Abstract\n",
        "\n",
        "This project investigates time series forecasting of vehicle registration trends in the German automotive market using neural network architectures. Leveraging monthly registration data from the Kraftfahrt-Bundesamt (KBA) spanning 2018–2025, we analyze registration patterns across multiple Original Equipment Manufacturers (OEMs), vehicle models, and powertrain types (Electric, Hybrid, Diesel, Petrol). The primary objective is to develop robust forecasting models capable of predicting monthly future registration volumes at the most granular level—individual model-powertrain combinations—thereby providing insights into the evolving landscape of automotive mobility.\n",
        "\n",
        "# Notebook (1/3): Univariate Timeseries Using One-Hot-Encoding\n",
        "\n",
        "This notebook uses the approach One-Hot-Encoding to feed the timeseries data to the Deep Learning Models.\n",
        "\n",
        "### TimeSeriesDataset One-Hot\n",
        "```\n",
        "Paradigm: One sample per series per time window\n",
        "Philosophy: Treat each time series independently with identity encoding\n",
        "Batching: Mix different series and time windows randomly\n",
        "```\n",
        "\n",
        "**Key characteristics:**\n",
        "- Each sample represents ONE time series at ONE time window\n",
        "- Series identity encoded as one-hot vector (1000 dimensions)\n",
        "- Supports multivariate features\n",
        "- Most flexible but least efficient\n",
        "\n",
        "Given: 1000 series × 60 timesteps, seq_length=6, embargo=1, and test_period=3 months, we have:\n",
        "\n",
        "**Total available time windows (before train-test split):**\n",
        "```\n",
        "n_windows_total = n_timesteps - seq_length - embargo - test_period\n",
        "                = 60 - 6 - 1 - 3\n",
        "                = 50 total windows\n",
        "```\n",
        "\n",
        "This number of windows represent the following number of training samples:\n",
        "\n",
        "```python\n",
        "# Training set\n",
        "dataset_length_train = n_series × n_windows_train\n",
        "                     = 1000 × 50\n",
        "                     = 50,000 training samples\n",
        "\n",
        "```\n",
        "\n",
        "This could be series #471 at time window #26:\n",
        "\n",
        "```python\n",
        "X.shape = (seq_length, n_features_total)\n",
        "        = (6, 1 + 3 + 2 + 1000)\n",
        "        = (6, 1006)\n",
        "\n",
        "# Feature composition per timestep:\n",
        "# [Value, year, month, GDP, CPI, Interest_Rate, onehot_0, ..., onehot_999]\n",
        "\n",
        "y.shape = (1,)\n",
        "\n",
        "# Example X (timestep t-5 to t):\n",
        "[\n",
        "  [1234.5, 2.3, 105.2, 3.5, 2023, 1, 0, 0, ..., 1, ..., 0],  # t-5\n",
        "  [1245.2, 2.3, 105.3, 3.5, 2023, 2, 0, 0, ..., 1, ..., 0],  # t-4\n",
        "  [1256.8, 2.4, 105.4, 3.6, 2023, 3, 0, 0, ..., 1, ..., 0],  # t-3\n",
        "  [1267.3, 2.4, 105.5, 3.6, 2023, 4, 0, 0, ..., 1, ..., 0],  # t-2\n",
        "  [1278.9, 2.5, 105.6, 3.7, 2023, 5, 0, 0, ..., 1, ..., 0],  # t-1\n",
        "  [1289.1, 2.5, 105.7, 3.7, 2023, 6, 0, 0, ..., 1, ..., 0]   # t\n",
        "]\n",
        "\n",
        "# Target y (timestep t+1+embargo):\n",
        "[1302.4]\n",
        "```\n",
        "\n",
        "### Batch Structure\n",
        "\n",
        "#### TimeSeriesDataset (batch_size=32)\n",
        "\n",
        "```python\n",
        "X_batch.shape = (batch_size, seq_length, n_features_total)\n",
        "              = (32, 6, 1006)\n",
        "\n",
        "y_batch.shape = (batch_size, 1)\n",
        "              = (32, 1)\n",
        "\n",
        "# This batch contains 32 random samples\n",
        "# Could be: series 42 window 10, series 721 window 33, series 5 window 18, ...\n",
        "# Each sample is from ONE series at ONE time window\n",
        "```\n",
        "\n",
        "**Interpretation:**\n",
        "- 32 independent predictions\n",
        "- Mixed series and time windows\n",
        "- Each row: one series' sequence with its one-hot encoding"
      ],
      "metadata": {
        "id": "S_qC61eCdXV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugKs9waDzVdz",
        "outputId": "912f7749-ab60-41ce-9650-71079058560e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GLOBAL_PATH = \"/content/drive/MyDrive/HDM/\""
      ],
      "metadata": {
        "id": "DwBFFWeTAS8K"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TpIdvILPk2uf"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "path = \"/content/drive/MyDrive/HDM/data/gold/monthly_registration_volume_gold.parquet\"\n",
        "df = pd.read_parquet(path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "FqyuMU0yzf7S",
        "outputId": "8a3b8b64-89e8-41fd-c9e6-eff7fdac5c25"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Date                             ts_key  Value  Employment_Level  \\\n",
              "0 2018-01-31  ALFA ROMEO_GIULIA_All_Wheel_Drive   54.0          32504413   \n",
              "1 2018-02-28  ALFA ROMEO_GIULIA_All_Wheel_Drive   23.0          32550748   \n",
              "2 2018-03-31  ALFA ROMEO_GIULIA_All_Wheel_Drive   85.0          32660492   \n",
              "3 2018-04-30  ALFA ROMEO_GIULIA_All_Wheel_Drive   51.0          32782173   \n",
              "4 2018-05-31  ALFA ROMEO_GIULIA_All_Wheel_Drive   77.0          32857311   \n",
              "\n",
              "   Employment_Level_Change  Employment_Level_Relative_Change  \\\n",
              "0                -104455.0                          -0.00320   \n",
              "1                  46335.0                           0.00143   \n",
              "2                 109744.0                           0.00337   \n",
              "3                 121681.0                           0.00373   \n",
              "4                  75138.0                           0.00229   \n",
              "\n",
              "   Employment_Level_YoY_Change  Employment_Level_YoY_Relative_Change  \\\n",
              "0                     796982.0                               0.02514   \n",
              "1                     776950.0                               0.02445   \n",
              "2                     730269.0                               0.02287   \n",
              "3                     768929.0                               0.02402   \n",
              "4                     725968.0                               0.02259   \n",
              "\n",
              "   Employment_Level_MA_3  Employment_Level_MA_6  ...     GDP  Lending_Rate  \\\n",
              "0           3.264763e+07           3.264138e+07  ...  104.34          0.25   \n",
              "1           3.255468e+07           3.266715e+07  ...  104.34          0.25   \n",
              "2           3.257188e+07           3.265527e+07  ...  102.81          0.25   \n",
              "3           3.266447e+07           3.265605e+07  ...  102.81          0.25   \n",
              "4           3.276666e+07           3.266067e+07  ...  102.81          0.25   \n",
              "\n",
              "   Lending_Rate_Change  Lending_Rate_Relative_Change  Lending_Rate_YoY_Change  \\\n",
              "0                  0.0                           0.0                      0.0   \n",
              "1                  0.0                           0.0                      0.0   \n",
              "2                  0.0                           0.0                      0.0   \n",
              "3                  0.0                           0.0                      0.0   \n",
              "4                  0.0                           0.0                      0.0   \n",
              "\n",
              "   Lending_Rate_YoY_Relative_Change  GR_price_with_tax_euro95  \\\n",
              "0                               0.0                    1550.0   \n",
              "1                               0.0                    1550.0   \n",
              "2                               0.0                    1533.0   \n",
              "3                               0.0                    1575.0   \n",
              "4                               0.0                    1647.0   \n",
              "\n",
              "   GR_price_with_tax_diesel  GR_price_with_tax_heGRing_oil  \\\n",
              "0                    1326.0                         1018.0   \n",
              "1                    1325.0                         1010.0   \n",
              "2                    1309.0                          991.0   \n",
              "3                    1348.0                         1025.0   \n",
              "4                    1420.0                         1025.0   \n",
              "\n",
              "   GR_price_with_tax_fuel_oil_1  \n",
              "0                        442.14  \n",
              "1                        424.16  \n",
              "2                        431.53  \n",
              "3                        453.57  \n",
              "4                        520.12  \n",
              "\n",
              "[5 rows x 27 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dd8f010f-9c85-4668-ab6a-8c60f0f65a53\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>ts_key</th>\n",
              "      <th>Value</th>\n",
              "      <th>Employment_Level</th>\n",
              "      <th>Employment_Level_Change</th>\n",
              "      <th>Employment_Level_Relative_Change</th>\n",
              "      <th>Employment_Level_YoY_Change</th>\n",
              "      <th>Employment_Level_YoY_Relative_Change</th>\n",
              "      <th>Employment_Level_MA_3</th>\n",
              "      <th>Employment_Level_MA_6</th>\n",
              "      <th>...</th>\n",
              "      <th>GDP</th>\n",
              "      <th>Lending_Rate</th>\n",
              "      <th>Lending_Rate_Change</th>\n",
              "      <th>Lending_Rate_Relative_Change</th>\n",
              "      <th>Lending_Rate_YoY_Change</th>\n",
              "      <th>Lending_Rate_YoY_Relative_Change</th>\n",
              "      <th>GR_price_with_tax_euro95</th>\n",
              "      <th>GR_price_with_tax_diesel</th>\n",
              "      <th>GR_price_with_tax_heGRing_oil</th>\n",
              "      <th>GR_price_with_tax_fuel_oil_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-01-31</td>\n",
              "      <td>ALFA ROMEO_GIULIA_All_Wheel_Drive</td>\n",
              "      <td>54.0</td>\n",
              "      <td>32504413</td>\n",
              "      <td>-104455.0</td>\n",
              "      <td>-0.00320</td>\n",
              "      <td>796982.0</td>\n",
              "      <td>0.02514</td>\n",
              "      <td>3.264763e+07</td>\n",
              "      <td>3.264138e+07</td>\n",
              "      <td>...</td>\n",
              "      <td>104.34</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1550.0</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>1018.0</td>\n",
              "      <td>442.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-02-28</td>\n",
              "      <td>ALFA ROMEO_GIULIA_All_Wheel_Drive</td>\n",
              "      <td>23.0</td>\n",
              "      <td>32550748</td>\n",
              "      <td>46335.0</td>\n",
              "      <td>0.00143</td>\n",
              "      <td>776950.0</td>\n",
              "      <td>0.02445</td>\n",
              "      <td>3.255468e+07</td>\n",
              "      <td>3.266715e+07</td>\n",
              "      <td>...</td>\n",
              "      <td>104.34</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1550.0</td>\n",
              "      <td>1325.0</td>\n",
              "      <td>1010.0</td>\n",
              "      <td>424.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-03-31</td>\n",
              "      <td>ALFA ROMEO_GIULIA_All_Wheel_Drive</td>\n",
              "      <td>85.0</td>\n",
              "      <td>32660492</td>\n",
              "      <td>109744.0</td>\n",
              "      <td>0.00337</td>\n",
              "      <td>730269.0</td>\n",
              "      <td>0.02287</td>\n",
              "      <td>3.257188e+07</td>\n",
              "      <td>3.265527e+07</td>\n",
              "      <td>...</td>\n",
              "      <td>102.81</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1533.0</td>\n",
              "      <td>1309.0</td>\n",
              "      <td>991.0</td>\n",
              "      <td>431.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-04-30</td>\n",
              "      <td>ALFA ROMEO_GIULIA_All_Wheel_Drive</td>\n",
              "      <td>51.0</td>\n",
              "      <td>32782173</td>\n",
              "      <td>121681.0</td>\n",
              "      <td>0.00373</td>\n",
              "      <td>768929.0</td>\n",
              "      <td>0.02402</td>\n",
              "      <td>3.266447e+07</td>\n",
              "      <td>3.265605e+07</td>\n",
              "      <td>...</td>\n",
              "      <td>102.81</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>1348.0</td>\n",
              "      <td>1025.0</td>\n",
              "      <td>453.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-05-31</td>\n",
              "      <td>ALFA ROMEO_GIULIA_All_Wheel_Drive</td>\n",
              "      <td>77.0</td>\n",
              "      <td>32857311</td>\n",
              "      <td>75138.0</td>\n",
              "      <td>0.00229</td>\n",
              "      <td>725968.0</td>\n",
              "      <td>0.02259</td>\n",
              "      <td>3.276666e+07</td>\n",
              "      <td>3.266067e+07</td>\n",
              "      <td>...</td>\n",
              "      <td>102.81</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1647.0</td>\n",
              "      <td>1420.0</td>\n",
              "      <td>1025.0</td>\n",
              "      <td>520.12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 27 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd8f010f-9c85-4668-ab6a-8c60f0f65a53')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dd8f010f-9c85-4668-ab6a-8c60f0f65a53 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dd8f010f-9c85-4668-ab6a-8c60f0f65a53');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f82e1519-e9b6-4942-949e-c812761e79e7\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f82e1519-e9b6-4942-949e-c812761e79e7')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f82e1519-e9b6-4942-949e-c812761e79e7 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/john2408/neural_networks_project_hdm.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9zJfG-StP580",
        "outputId": "98bfc4c4-5cef-4ec2-ce65-0314ab572069"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/john2408/neural_networks_project_hdm.git\n",
            "  Cloning https://github.com/john2408/neural_networks_project_hdm.git to /tmp/pip-req-build-jgfr3ghh\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/john2408/neural_networks_project_hdm.git /tmp/pip-req-build-jgfr3ghh\n",
            "  Resolved https://github.com/john2408/neural_networks_project_hdm.git to commit 22c0b7d8e572fa4ef6028278ecf51a089e694014\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting black>=25.12.0 (from neuralts==0.0.1)\n",
            "  Downloading black-25.12.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.4/86.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flake8>=7.3.0 (from neuralts==0.0.1)\n",
            "  Downloading flake8-7.3.0-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting isort>=7.0.0 (from neuralts==0.0.1)\n",
            "  Downloading isort-7.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting jupyter-book<2 (from neuralts==0.0.1)\n",
            "  Downloading jupyter_book-1.0.4.post1-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting jupyterlab>=4.5.0 (from neuralts==0.0.1)\n",
            "  Downloading jupyterlab-4.5.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting neuralforecast>=3.1.2 (from neuralts==0.0.1)\n",
            "  Downloading neuralforecast-3.1.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: openpyxl>=3.1.5 in /usr/local/lib/python3.12/dist-packages (from neuralts==0.0.1) (3.1.5)\n",
            "Collecting optuna>=4.6.0 (from neuralts==0.0.1)\n",
            "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting pandas>=2.3.3 (from neuralts==0.0.1)\n",
            "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarrow>=22.0.0 (from neuralts==0.0.1)\n",
            "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting scikit-learn>=1.7.2 (from neuralts==0.0.1)\n",
            "  Downloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: seaborn>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from neuralts==0.0.1) (0.13.2)\n",
            "Collecting torch>=2.9.1 (from neuralts==0.0.1)\n",
            "  Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Requirement already satisfied: xlrd>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from neuralts==0.0.1) (2.0.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from black>=25.12.0->neuralts==0.0.1) (8.3.1)\n",
            "Collecting mypy-extensions>=0.4.3 (from black>=25.12.0->neuralts==0.0.1)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.12/dist-packages (from black>=25.12.0->neuralts==0.0.1) (25.0)\n",
            "Collecting pathspec>=0.9.0 (from black>=25.12.0->neuralts==0.0.1)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.12/dist-packages (from black>=25.12.0->neuralts==0.0.1) (4.5.1)\n",
            "Collecting pytokens>=0.3.0 (from black>=25.12.0->neuralts==0.0.1)\n",
            "  Downloading pytokens-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting mccabe<0.8.0,>=0.7.0 (from flake8>=7.3.0->neuralts==0.0.1)\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting pycodestyle<2.15.0,>=2.14.0 (from flake8>=7.3.0->neuralts==0.0.1)\n",
            "  Downloading pycodestyle-2.14.0-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting pyflakes<3.5.0,>=3.4.0 (from flake8>=7.3.0->neuralts==0.0.1)\n",
            "  Downloading pyflakes-3.4.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from jupyter-book<2->neuralts==0.0.1) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5 in /usr/local/lib/python3.12/dist-packages (from jupyter-book<2->neuralts==0.0.1) (4.25.1)\n",
            "Requirement already satisfied: linkify-it-py<3,>=2 in /usr/local/lib/python3.12/dist-packages (from jupyter-book<2->neuralts==0.0.1) (2.0.3)\n",
            "Collecting myst-nb~=1.0 (from jupyter-book<2->neuralts==0.0.1)\n",
            "  Downloading myst_nb-1.3.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting myst-parser~=3.0 (from jupyter-book<2->neuralts==0.0.1)\n",
            "  Downloading myst_parser-3.0.1-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from jupyter-book<2->neuralts==0.0.1) (6.0.3)\n",
            "Collecting sphinx-book-theme~=1.1 (from jupyter-book<2->neuralts==0.0.1)\n",
            "  Downloading sphinx_book_theme-1.1.4-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting sphinx-comments~=0.0 (from jupyter-book<2->neuralts==0.0.1)\n",
            "  Downloading sphinx_comments-0.0.3-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting sphinx-copybutton~=0.5 (from jupyter-book<2->neuralts==0.0.1)\n",
            "  Downloading sphinx_copybutton-0.5.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting sphinx-design~=0.6 (from jupyter-book<2->neuralts==0.0.1)\n",
            "  Downloading sphinx_design-0.6.1-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting sphinx-external-toc~=1.0 (from jupyter-book<2->neuralts==0.0.1)\n",
            "  Downloading sphinx_external_toc-1.0.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting sphinx-jupyterbook-latex~=1.0 (from jupyter-book<2->neuralts==0.0.1)\n",
            "  Downloading sphinx_jupyterbook_latex-1.0.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting sphinx-multitoc-numbering~=0.1 (from jupyter-book<2->neuralts==0.0.1)\n",
            "  Downloading sphinx_multitoc_numbering-0.1.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting sphinx-thebe~=0.3 (from jupyter-book<2->neuralts==0.0.1)\n",
            "  Downloading sphinx_thebe-0.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting sphinx-togglebutton~=0.3 (from jupyter-book<2->neuralts==0.0.1)\n",
            "  Downloading sphinx_togglebutton-0.3.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting sphinxcontrib-bibtex~=2.5 (from jupyter-book<2->neuralts==0.0.1)\n",
            "  Downloading sphinxcontrib_bibtex-2.6.5-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting sphinx~=7.0 (from jupyter-book<2->neuralts==0.0.1)\n",
            "  Downloading sphinx-7.4.7-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab>=4.5.0->neuralts==0.0.1)\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab>=4.5.0->neuralts==0.0.1) (0.28.1)\n",
            "Requirement already satisfied: ipykernel!=6.30.0,>=6.5.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab>=4.5.0->neuralts==0.0.1) (6.17.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.12/dist-packages (from jupyterlab>=4.5.0->neuralts==0.0.1) (5.9.1)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab>=4.5.0->neuralts==0.0.1)\n",
            "  Downloading jupyter_lsp-2.3.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab>=4.5.0->neuralts==0.0.1) (2.14.0)\n",
            "Collecting jupyterlab-server<3,>=2.28.0 (from jupyterlab>=4.5.0->neuralts==0.0.1)\n",
            "  Downloading jupyterlab_server-2.28.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.12/dist-packages (from jupyterlab>=4.5.0->neuralts==0.0.1) (0.2.4)\n",
            "Requirement already satisfied: setuptools>=41.1.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab>=4.5.0->neuralts==0.0.1) (75.2.0)\n",
            "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab>=4.5.0->neuralts==0.0.1) (6.5.1)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.12/dist-packages (from jupyterlab>=4.5.0->neuralts==0.0.1) (5.7.1)\n",
            "Collecting coreforecast>=0.0.6 (from neuralforecast>=3.1.2->neuralts==0.0.1)\n",
            "  Downloading coreforecast-0.0.16-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from neuralforecast>=3.1.2->neuralts==0.0.1) (2025.3.0)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from neuralforecast>=3.1.2->neuralts==0.0.1) (2.0.2)\n",
            "Collecting pytorch-lightning>=2.0.0 (from neuralforecast>=3.1.2->neuralts==0.0.1)\n",
            "  Downloading pytorch_lightning-2.6.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting ray>=2.2.0 (from ray[tune]>=2.2.0->neuralforecast>=3.1.2->neuralts==0.0.1)\n",
            "  Downloading ray-2.53.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting utilsforecast>=0.2.3 (from neuralforecast>=3.1.2->neuralts==0.0.1)\n",
            "  Downloading utilsforecast-0.2.15-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl>=3.1.5->neuralts==0.0.1) (2.0.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna>=4.6.0->neuralts==0.0.1) (1.17.2)\n",
            "Collecting colorlog (from optuna>=4.6.0->neuralts==0.0.1)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna>=4.6.0->neuralts==0.0.1) (2.0.45)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna>=4.6.0->neuralts==0.0.1) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.3.3->neuralts==0.0.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.3.3->neuralts==0.0.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.3.3->neuralts==0.0.1) (2025.3)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.7.2->neuralts==0.0.1) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.7.2->neuralts==0.0.1) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.7.2->neuralts==0.0.1) (3.6.0)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.12/dist-packages (from seaborn>=0.13.2->neuralts==0.0.1) (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.9.1->neuralts==0.0.1) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.9.1->neuralts==0.0.1) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.9.1->neuralts==0.0.1) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.9.1->neuralts==0.0.1) (3.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=2.9.1->neuralts==0.0.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=2.9.1->neuralts==0.0.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=2.9.1->neuralts==0.0.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.9.1->neuralts==0.0.1) (9.10.2.21)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=2.9.1->neuralts==0.0.1)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=2.9.1->neuralts==0.0.1)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=2.9.1->neuralts==0.0.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=2.9.1->neuralts==0.0.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=2.9.1->neuralts==0.0.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.9.1->neuralts==0.0.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.9.1->neuralts==0.0.1) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.9.1->neuralts==0.0.1) (3.3.20)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=2.9.1->neuralts==0.0.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=2.9.1->neuralts==0.0.1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=2.9.1->neuralts==0.0.1)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.5.1 (from torch>=2.9.1->neuralts==0.0.1)\n",
            "  Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna>=4.6.0->neuralts==0.0.1) (1.3.10)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab>=4.5.0->neuralts==0.0.1) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab>=4.5.0->neuralts==0.0.1) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab>=4.5.0->neuralts==0.0.1) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab>=4.5.0->neuralts==0.0.1) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab>=4.5.0->neuralts==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab>=4.5.0->neuralts==0.0.1) (1.8.15)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab>=4.5.0->neuralts==0.0.1) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab>=4.5.0->neuralts==0.0.1) (7.4.9)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab>=4.5.0->neuralts==0.0.1) (0.2.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab>=4.5.0->neuralts==0.0.1) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab>=4.5.0->neuralts==0.0.1) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab>=4.5.0->neuralts==0.0.1) (26.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->jupyter-book<2->neuralts==0.0.1) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5->jupyter-book<2->neuralts==0.0.1) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5->jupyter-book<2->neuralts==0.0.1) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5->jupyter-book<2->neuralts==0.0.1) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5->jupyter-book<2->neuralts==0.0.1) (0.30.0)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (25.1.0)\n",
            "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (0.5.3)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (7.16.6)\n",
            "Requirement already satisfied: nbformat>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (5.10.4)\n",
            "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (7.7.0)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (0.23.1)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (0.18.1)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (1.9.0)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab>=4.5.0->neuralts==0.0.1) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.28.0->jupyterlab>=4.5.0->neuralts==0.0.1)\n",
            "  Downloading json5-0.12.1-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab>=4.5.0->neuralts==0.0.1) (2.32.4)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.12/dist-packages (from linkify-it-py<3,>=2->jupyter-book<2->neuralts==0.0.1) (1.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.13.2->neuralts==0.0.1) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.13.2->neuralts==0.0.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.13.2->neuralts==0.0.1) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.13.2->neuralts==0.0.1) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.13.2->neuralts==0.0.1) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.13.2->neuralts==0.0.1) (3.2.5)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from myst-nb~=1.0->jupyter-book<2->neuralts==0.0.1) (8.7.0)\n",
            "Collecting jupyter-cache>=0.5 (from myst-nb~=1.0->jupyter-book<2->neuralts==0.0.1)\n",
            "  Downloading jupyter_cache-1.0.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: nbclient in /usr/local/lib/python3.12/dist-packages (from myst-nb~=1.0->jupyter-book<2->neuralts==0.0.1) (0.10.2)\n",
            "Requirement already satisfied: docutils<0.22,>=0.18 in /usr/local/lib/python3.12/dist-packages (from myst-parser~=3.0->jupyter-book<2->neuralts==0.0.1) (0.21.2)\n",
            "Collecting markdown-it-py~=3.0 (from myst-parser~=3.0->jupyter-book<2->neuralts==0.0.1)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: mdit-py-plugins~=0.4 in /usr/local/lib/python3.12/dist-packages (from myst-parser~=3.0->jupyter-book<2->neuralts==0.0.1) (0.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.3.3->neuralts==0.0.1) (1.17.0)\n",
            "Collecting torchmetrics>0.7.0 (from pytorch-lightning>=2.0.0->neuralforecast>=3.1.2->neuralts==0.0.1)\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning>=2.0.0->neuralforecast>=3.1.2->neuralts==0.0.1)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.2.0->ray[tune]>=2.2.0->neuralforecast>=3.1.2->neuralts==0.0.1) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from ray>=2.2.0->ray[tune]>=2.2.0->neuralforecast>=3.1.2->neuralts==0.0.1) (5.29.5)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.10.*,!=2.11.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3 in /usr/local/lib/python3.12/dist-packages (from ray[tune]>=2.2.0->neuralforecast>=3.1.2->neuralts==0.0.1) (2.12.3)\n",
            "Collecting tensorboardX>=1.9 (from ray[tune]>=2.2.0->neuralforecast>=3.1.2->neuralts==0.0.1)\n",
            "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.12/dist-packages (from sphinx~=7.0->jupyter-book<2->neuralts==0.0.1) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.12/dist-packages (from sphinx~=7.0->jupyter-book<2->neuralts==0.0.1) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.12/dist-packages (from sphinx~=7.0->jupyter-book<2->neuralts==0.0.1) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from sphinx~=7.0->jupyter-book<2->neuralts==0.0.1) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.12/dist-packages (from sphinx~=7.0->jupyter-book<2->neuralts==0.0.1) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.12/dist-packages (from sphinx~=7.0->jupyter-book<2->neuralts==0.0.1) (2.0.0)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.12/dist-packages (from sphinx~=7.0->jupyter-book<2->neuralts==0.0.1) (2.19.2)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.12/dist-packages (from sphinx~=7.0->jupyter-book<2->neuralts==0.0.1) (3.0.1)\n",
            "Collecting alabaster~=0.7.14 (from sphinx~=7.0->jupyter-book<2->neuralts==0.0.1)\n",
            "  Downloading alabaster-0.7.16-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.12/dist-packages (from sphinx~=7.0->jupyter-book<2->neuralts==0.0.1) (1.4.1)\n",
            "Collecting pydata-sphinx-theme==0.15.4 (from sphinx-book-theme~=1.1->jupyter-book<2->neuralts==0.0.1)\n",
            "  Downloading pydata_sphinx_theme-0.15.4-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from pydata-sphinx-theme==0.15.4->sphinx-book-theme~=1.1->jupyter-book<2->neuralts==0.0.1) (4.13.5)\n",
            "Collecting accessible-pygments (from pydata-sphinx-theme==0.15.4->sphinx-book-theme~=1.1->jupyter-book<2->neuralts==0.0.1)\n",
            "  Downloading accessible_pygments-0.0.5-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (from sphinx-togglebutton~=0.3->jupyter-book<2->neuralts==0.0.1) (0.45.1)\n",
            "Collecting pybtex>=0.25 (from sphinxcontrib-bibtex~=2.5->jupyter-book<2->neuralts==0.0.1)\n",
            "  Downloading pybtex-0.25.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pybtex-docutils>=1.0.0 (from sphinxcontrib-bibtex~=2.5->jupyter-book<2->neuralts==0.0.1)\n",
            "  Downloading pybtex_docutils-1.0.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna>=4.6.0->neuralts==0.0.1) (3.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.9.1->neuralts==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: narwhals>=2.0 in /usr/local/lib/python3.12/dist-packages (from utilsforecast>=0.2.3->neuralforecast>=3.1.2->neuralts==0.0.1) (2.13.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (25.1.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->neuralforecast>=3.1.2->neuralts==0.0.1) (3.13.2)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab>=4.5.0->neuralts==0.0.1)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab>=4.5.0->neuralts==0.0.1) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab>=4.5.0->neuralts==0.0.1) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab>=4.5.0->neuralts==0.0.1) (3.0.52)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab>=4.5.0->neuralts==0.0.1) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab>=4.5.0->neuralts==0.0.1) (4.9.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from jupyter-cache>=0.5->myst-nb~=1.0->jupyter-book<2->neuralts==0.0.1) (0.9.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel!=6.30.0,>=6.5.0->jupyterlab>=4.5.0->neuralts==0.0.1) (0.4)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (4.0.0)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (0.1.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py~=3.0->myst-parser~=3.0->jupyter-book<2->neuralts==0.0.1) (0.1.2)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (6.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (3.1.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (2.21.2)\n",
            "Collecting latexcodec>=1.0.4 (from pybtex>=0.25->sphinxcontrib-bibtex~=2.5->jupyter-book<2->neuralts==0.0.1)\n",
            "  Downloading latexcodec-3.0.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.10.*,!=2.11.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3->ray[tune]>=2.2.0->neuralforecast>=3.1.2->neuralts==0.0.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.10.*,!=2.11.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3->ray[tune]>=2.2.0->neuralforecast>=3.1.2->neuralts==0.0.1) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.10.*,!=2.11.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3->ray[tune]>=2.2.0->neuralforecast>=3.1.2->neuralts==0.0.1) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.28.0->jupyterlab>=4.5.0->neuralts==0.0.1) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.28.0->jupyterlab>=4.5.0->neuralts==0.0.1) (2.5.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.12/dist-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (0.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->myst-nb~=1.0->jupyter-book<2->neuralts==0.0.1) (3.23.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->neuralforecast>=3.1.2->neuralts==0.0.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->neuralforecast>=3.1.2->neuralts==0.0.1) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->neuralforecast>=3.1.2->neuralts==0.0.1) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->neuralforecast>=3.1.2->neuralts==0.0.1) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->neuralforecast>=3.1.2->neuralts==0.0.1) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->neuralforecast>=3.1.2->neuralts==0.0.1) (1.22.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (1.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab>=4.5.0->neuralts==0.0.1) (0.8.5)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (3.0.0)\n",
            "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (25.10.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab>=4.5.0->neuralts==0.0.1) (0.2.14)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (2.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->pydata-sphinx-theme==0.15.4->sphinx-book-theme~=1.1->jupyter-book<2->neuralts==0.0.1) (2.8)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (2.23)\n",
            "Requirement already satisfied: lark>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (1.3.1)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.5.0->neuralts==0.0.1) (1.4.0)\n",
            "Downloading black-25.12.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flake8-7.3.0-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isort-7.0.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_book-1.0.4.post1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab-4.5.1-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m151.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading neuralforecast-3.1.2-py3-none-any.whl (263 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.2/263.2 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m139.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m144.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m161.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading coreforecast-0.0.16-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.4/287.4 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_lsp-2.3.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.28.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.8/59.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading myst_nb-1.3.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.4/82.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading myst_parser-3.0.1-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading pycodestyle-2.14.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading pyflakes-3.4.0-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytokens-0.3.0-py3-none-any.whl (12 kB)\n",
            "Downloading pytorch_lightning-2.6.0-py3-none-any.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.53.0-cp312-cp312-manylinux2014_x86_64.whl (72.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx-7.4.7-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m126.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx_book_theme-1.1.4-py3-none-any.whl (433 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.0/434.0 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydata_sphinx_theme-0.15.4-py3-none-any.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m128.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx_comments-0.0.3-py3-none-any.whl (4.6 kB)\n",
            "Downloading sphinx_copybutton-0.5.2-py3-none-any.whl (13 kB)\n",
            "Downloading sphinx_design-0.6.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx_external_toc-1.0.1-py3-none-any.whl (26 kB)\n",
            "Downloading sphinx_jupyterbook_latex-1.0.0-py3-none-any.whl (13 kB)\n",
            "Downloading sphinx_multitoc_numbering-0.1.3-py3-none-any.whl (4.6 kB)\n",
            "Downloading sphinx_thebe-0.3.1-py3-none-any.whl (9.0 kB)\n",
            "Downloading sphinx_togglebutton-0.3.2-py3-none-any.whl (8.2 kB)\n",
            "Downloading sphinxcontrib_bibtex-2.6.5-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading utilsforecast-0.2.15-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Downloading alabaster-0.7.16-py3-none-any.whl (13 kB)\n",
            "Downloading json5-0.12.1-py3-none-any.whl (36 kB)\n",
            "Downloading jupyter_cache-1.0.1-py3-none-any.whl (33 kB)\n",
            "Downloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybtex-0.25.1-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybtex_docutils-1.0.3-py3-none-any.whl (6.4 kB)\n",
            "Downloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading latexcodec-3.0.1-py3-none-any.whl (18 kB)\n",
            "Downloading accessible_pygments-0.0.5-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: neuralts\n",
            "  Building wheel for neuralts (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neuralts: filename=neuralts-0.0.1-py3-none-any.whl size=44437 sha256=5c98601f0a252c38770150c5887c0d30e616c4103dca6af215d6c7c0f95c9ee1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-s5goe8bk/wheels/51/e1/6e/1d29e227c6bbb6075653cb46b6f2ab14a3f26bf22d1ba7a6a4\n",
            "Successfully built neuralts\n",
            "Installing collected packages: triton, tensorboardX, pytokens, pyflakes, pycodestyle, pyarrow, pathspec, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, mccabe, markdown-it-py, lightning-utilities, latexcodec, json5, jedi, isort, coreforecast, colorlog, async-lru, alabaster, accessible-pygments, sphinx, scikit-learn, pybtex, pandas, nvidia-cusparse-cu12, nvidia-cufft-cu12, flake8, black, utilsforecast, sphinx-togglebutton, sphinx-thebe, sphinx-multitoc-numbering, sphinx-jupyterbook-latex, sphinx-external-toc, sphinx-design, sphinx-copybutton, sphinx-comments, pydata-sphinx-theme, pybtex-docutils, optuna, nvidia-cusolver-cu12, myst-parser, torch, sphinxcontrib-bibtex, sphinx-book-theme, ray, torchmetrics, pytorch-lightning, jupyter-cache, neuralforecast, myst-nb, jupyterlab-server, jupyter-lsp, jupyter-book, jupyterlab, neuralts\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.5.0\n",
            "    Uninstalling triton-3.5.0:\n",
            "      Successfully uninstalled triton-3.5.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufile-cu12\n",
            "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
            "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
            "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 4.0.0\n",
            "    Uninstalling markdown-it-py-4.0.0:\n",
            "      Successfully uninstalled markdown-it-py-4.0.0\n",
            "  Attempting uninstall: alabaster\n",
            "    Found existing installation: alabaster 1.0.0\n",
            "    Uninstalling alabaster-1.0.0:\n",
            "      Successfully uninstalled alabaster-1.0.0\n",
            "  Attempting uninstall: sphinx\n",
            "    Found existing installation: Sphinx 8.2.3\n",
            "    Uninstalling Sphinx-8.2.3:\n",
            "      Successfully uninstalled Sphinx-8.2.3\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.0+cu126\n",
            "    Uninstalling torch-2.9.0+cu126:\n",
            "      Successfully uninstalled torch-2.9.0+cu126\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accessible-pygments-0.0.5 alabaster-0.7.16 async-lru-2.0.5 black-25.12.0 colorlog-6.10.1 coreforecast-0.0.16 flake8-7.3.0 isort-7.0.0 jedi-0.19.2 json5-0.12.1 jupyter-book-1.0.4.post1 jupyter-cache-1.0.1 jupyter-lsp-2.3.0 jupyterlab-4.5.1 jupyterlab-server-2.28.0 latexcodec-3.0.1 lightning-utilities-0.15.2 markdown-it-py-3.0.0 mccabe-0.7.0 mypy-extensions-1.1.0 myst-nb-1.3.0 myst-parser-3.0.1 neuralforecast-3.1.2 neuralts-0.0.1 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 optuna-4.6.0 pandas-2.3.3 pathspec-0.12.1 pyarrow-22.0.0 pybtex-0.25.1 pybtex-docutils-1.0.3 pycodestyle-2.14.0 pydata-sphinx-theme-0.15.4 pyflakes-3.4.0 pytokens-0.3.0 pytorch-lightning-2.6.0 ray-2.53.0 scikit-learn-1.8.0 sphinx-7.4.7 sphinx-book-theme-1.1.4 sphinx-comments-0.0.3 sphinx-copybutton-0.5.2 sphinx-design-0.6.1 sphinx-external-toc-1.0.1 sphinx-jupyterbook-latex-1.0.0 sphinx-multitoc-numbering-0.1.3 sphinx-thebe-0.3.1 sphinx-togglebutton-0.3.2 sphinxcontrib-bibtex-2.6.5 tensorboardX-2.6.4 torch-2.9.1 torchmetrics-1.8.2 triton-3.5.1 utilsforecast-0.2.15\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas",
                  "pyarrow",
                  "sphinxcontrib"
                ]
              },
              "id": "2266fe45a2614c8fadf2ee79ecedcc44"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "from neuralts.core.models import (LSTMForecaster, RNNForecaster, GRUForecaster,\n",
        "                            CNN1DForecaster, MLPForecaster, TransformerForecaster, TransformerForecasterCLS)\n",
        "from neuralts.core.metrics import smape, calculate_smape_distribution\n",
        "from neuralts.core.func import TimeSeriesDataset, generate_out_of_sample_predictions, train_epoch, evaluate\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "    # ========================================================================\n",
        "    # TRAINING PARAMETERS\n",
        "    # ========================================================================\n",
        "\n",
        "    MODEL = 'RNN'  # Options: 'LSTM', 'RNN', 'GRU', 'CNN1D', 'MLP', 'Transformer', 'BASELINE'\n",
        "    MODE = \"UNI\"  # Options: \"UNI\": Univariate (Only Month and Year), \"EXO\": Exogenous Variable (All features)\n",
        "\n",
        "    PYTORCH_SEED = 42\n",
        "    SEQ_LENGTH = 6\n",
        "    TRAIN_RATIO = 0.8\n",
        "    EMBARGO = 1\n",
        "    EPOCHS = 25\n",
        "    BATCH_SIZE = 64\n",
        "    LEARNING_RATE = 0.001\n",
        "    WEIGHT_DECAY = 1e-5\n",
        "\n",
        "    MLP_LAYERS = 2\n",
        "    MLP_HIDDEN_SIZE = 512\n",
        "    MLP_DROPOUT = 0.2\n",
        "\n",
        "    LSTM_LAYERS = 2\n",
        "    LSTM_HIDDEN_SIZE = 128\n",
        "    LSTM_DROPOUT = 0.2\n",
        "\n",
        "    RNN_LAYERS = 2\n",
        "    RNN_HIDDEN_SIZE = 128\n",
        "    RNN_DROPOUT = 0.2\n",
        "\n",
        "    GRU_LAYERS = 2\n",
        "    GRU_HIDDEN_SIZE = 128\n",
        "    GRU_DROPOUT = 0.2\n",
        "\n",
        "    CNN_LAYERS = 3\n",
        "    CNN_HIDDEN_SIZE = 64\n",
        "    CNN_DROPOUT = 0.2\n",
        "\n",
        "    TRANSFORMER_D_MODEL = 64\n",
        "    TRANSFORMER_NHEAD = 4\n",
        "    TRANSFORMER_LAYERS = 2\n",
        "    TRANSFORMER_DIM_FEEDFORWARD = 256\n",
        "    TRANSFORMER_DROPOUT = 0.2\n",
        "\n",
        "    # HYPERPARAMETER OPTIMIZATION WITH OPTUNA\n",
        "    OPTIMIZE_HYPERPARAMETERS = True  # Set to False to skip optimization\n",
        "    N_TRIALS = 3  # Number of Optuna trials\n",
        "    OPTUNA_TIMEOUT = 900  # Timeout in seconds (15 minutes)\n",
        "\n",
        "    TOTAL_SCRIPT_RUNTIME = None\n",
        "    TOTAL_TRAINING_TIME_FOLDS = dict()\n",
        "    TOTAL_OPTIMIZATION_TIME = None\n",
        "\n",
        "\n",
        "    OUTPUT_PATH = os.path.join(GLOBAL_PATH, \"models\", MODEL.lower() + \"_\" + MODE.lower() + \"_one_hot\")\n",
        "    os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "    df_path = os.path.join(GLOBAL_PATH, \"data\", \"gold\", \"monthly_registration_volume_gold.parquet\")\n",
        "\n",
        "    # ========================================================================\n",
        "\n",
        "    torch.manual_seed(PYTORCH_SEED)\n",
        "    np.random.seed(PYTORCH_SEED)\n",
        "    torch.cuda.manual_seed_all(PYTORCH_SEED)\n",
        "    torch.backends.cudnn.deterministic = True # Ensures reproducibility\n",
        "    torch.backends.cudnn.benchmark = False # Ensures reproducibility\n",
        "\n",
        "    SCRIPT_START_TIME = time.time()\n",
        "\n",
        "    # Check MPS availability\n",
        "    if torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        print(f\"✓ MPS device is available\")\n",
        "    elif torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(f\"✓ CUDA device is available\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(f\"Using CPU\")\n",
        "\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "\n",
        "\n",
        "    # Load data\n",
        "    df_full = pd.read_parquet(df_path)\n",
        "    df_full['Year'] = df_full['Date'].dt.year\n",
        "    df_full['Month'] = df_full['Date'].dt.month\n",
        "\n",
        "    date_col = 'Date'\n",
        "    ts_key_col = 'ts_key'\n",
        "    value_col = 'Value'\n",
        "    #features = [col for col in df_full.columns if col not in [date_col, ts_key_col, value_col]]\n",
        "\n",
        "    if MODE == \"UNI\":\n",
        "        features = ['Year', 'Month']\n",
        "    elif MODE == \"MULTI\":\n",
        "        features = [col for col in df_full.columns if col not in [date_col, ts_key_col, value_col]]\n",
        "\n",
        "    #Validate not NaN or infinite values in features\n",
        "    assert not df_full[features].isna().any().any(), \"NaN values found in features\"\n",
        "    assert not np.isinf(df_full[features].select_dtypes(include=[np.number])).any().any(), \"Infinite values found in features\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"DATA OVERVIEW\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Original data shape: {df_full.shape}\")\n",
        "    print(f\"Unique time series: {df_full.ts_key.nunique()}\")\n",
        "    print(f\"Date range: {df_full.Date.min()} to {df_full.Date.max()}\")\n",
        "    print(f\"Total observations: {len(df_full):,}\")\n",
        "\n",
        "\n",
        "    # Check original data\n",
        "    print(f\"\\nOriginal DataFrame:\")\n",
        "    print(f\"  NaN values: {df_full.isna().sum().sum()}\")\n",
        "    print(f\"  Inf values: {np.isinf(df_full.select_dtypes(include=[np.number])).sum().sum()}\")\n",
        "    print(f\"  Value range: [{df_full['Value'].min():.2f}, {df_full['Value'].max():.2f}]\")\n",
        "\n",
        "    # Check for zero/very small values that could cause division issues\n",
        "    print(f\"\\nZero values in 'Value' column: {(df_full['Value'] == 0).sum()}\")\n",
        "    print(f\"Values < 0.01: {(df_full['Value'] < 0.01).sum()}\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # FOLD CONFIGURATION - As per Problem Description\n",
        "    # ========================================================================\n",
        "\n",
        "    folds = [\n",
        "        {\n",
        "            'name': 'Fold 1',\n",
        "            'train_end': '2024-09-30',    # Train on data up to Sep 2024\n",
        "            'test_start': '2024-10-01',   # Test on Oct-Dec 2024\n",
        "            'test_end': '2024-12-31'\n",
        "        },\n",
        "        {\n",
        "            'name': 'Fold 2',\n",
        "            'train_end': '2024-12-31',    # Train on data up to Dec 2024\n",
        "            'test_start': '2025-01-01',   # Test on Jan-Mar 2025\n",
        "            'test_end': '2025-03-31'\n",
        "        },\n",
        "        {\n",
        "            'name': 'Fold 3',\n",
        "            'train_end': '2025-06-30',    # Train on data up to Jun 2025\n",
        "            'test_start': '2025-07-01',   # Test on Jul-Sep 2025\n",
        "            'test_end': '2025-09-30'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FOLD CONFIGURATION\")\n",
        "    print(\"=\"*80)\n",
        "    for fold in folds:\n",
        "        print(f\"\\n{fold['name']}:\")\n",
        "        print(f\"  Training data: up to {fold['train_end']}\")\n",
        "        print(f\"  Test period: {fold['test_start']} to {fold['test_end']}\")\n",
        "\n",
        "\n",
        "    # ========================================================================\n",
        "    # HYPERPARAMETER OPTIMIZATION WITH OPTUNA\n",
        "    # ========================================================================\n",
        "\n",
        "    best_trial = None\n",
        "    if OPTIMIZE_HYPERPARAMETERS and MODEL not in ['BASELINE']:\n",
        "        import optuna\n",
        "        from neuralts.core.func import create_univariate_model_from_trial, train_with_early_stopping\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"HYPERPARAMETER OPTIMIZATION WITH OPTUNA\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Model: {MODEL}\")\n",
        "        print(f\"Trials: {N_TRIALS}\")\n",
        "        print(f\"Timeout: {OPTUNA_TIMEOUT}s\")\n",
        "\n",
        "        # Use Fold 1 data for hyperparameter optimization\n",
        "        fold_config = folds[0]\n",
        "        df_train = df_full[df_full['Date'] <= fold_config['train_end']].copy()\n",
        "\n",
        "        print(f\"\\nUsing {fold_config['name']} for optimization\")\n",
        "        print(f\"  Training observations: {len(df_train):,}\")\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = TimeSeriesDataset(\n",
        "            df_train,\n",
        "            feature_cols=features,\n",
        "            seq_length=SEQ_LENGTH,\n",
        "            embargo=EMBARGO,\n",
        "            train=True,\n",
        "            train_ratio=TRAIN_RATIO\n",
        "        )\n",
        "\n",
        "        val_dataset = TimeSeriesDataset(\n",
        "            df_train,\n",
        "            feature_cols=features,\n",
        "            seq_length=SEQ_LENGTH,\n",
        "            train=False,\n",
        "            embargo=EMBARGO,\n",
        "            train_ratio=TRAIN_RATIO,\n",
        "            scaler_X=train_dataset.scaler_X,\n",
        "            scaler_y=train_dataset.scaler_y\n",
        "        )\n",
        "\n",
        "        INPUT_SIZE_OPT = train_dataset.X.shape[2]\n",
        "\n",
        "        print(f\"\\nOptimization dataset:\")\n",
        "        print(f\"  Training samples: {len(train_dataset):,}\")\n",
        "        print(f\"  Validation samples: {len(val_dataset):,}\")\n",
        "        print(f\"  Input size: {INPUT_SIZE_OPT}\")\n",
        "\n",
        "        def objective(trial):\n",
        "            \"\"\"\n",
        "            Optuna objective function for hyperparameter optimization.\n",
        "            \"\"\"\n",
        "            # Suggest hyperparameters\n",
        "            learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
        "            batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
        "\n",
        "            # Create model with trial hyperparameters\n",
        "            model, hyperparams = create_univariate_model_from_trial(\n",
        "                MODEL, trial, INPUT_SIZE_OPT, SEQ_LENGTH\n",
        "            )\n",
        "            model = model.to(device)\n",
        "\n",
        "            # Create data loaders\n",
        "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            # Train with early stopping\n",
        "            best_val_loss, _ = train_with_early_stopping(\n",
        "                model, train_loader, val_loader, device,\n",
        "                learning_rate=learning_rate,\n",
        "                weight_decay=WEIGHT_DECAY,\n",
        "                max_epochs=30,\n",
        "                patience=5\n",
        "            )\n",
        "\n",
        "            return best_val_loss\n",
        "\n",
        "        # Create and run study\n",
        "        study = optuna.create_study(\n",
        "            direction='minimize',\n",
        "            study_name=f'{MODEL}_optimization',\n",
        "            pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
        "        )\n",
        "\n",
        "        print(\"\\nStarting optimization...\")\n",
        "        OPTIMIZATION_START_TIME = time.time()\n",
        "        study.optimize(objective, n_trials=N_TRIALS, timeout=OPTUNA_TIMEOUT, show_progress_bar=True)\n",
        "        TOTAL_OPTIMIZATION_TIME = (time.time() - OPTIMIZATION_START_TIME) / 60.0  # in minutes\n",
        "        print(f\"\\nOptimization completed in {TOTAL_OPTIMIZATION_TIME:.1f} minutes\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"OPTIMIZATION RESULTS\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Number of finished trials: {len(study.trials)}\")\n",
        "        print(f\"\\nBest trial:\")\n",
        "        best_trial = study.best_trial\n",
        "        print(f\"  Value (Validation Loss): {best_trial.value:.6f}\")\n",
        "        print(f\"  Hyperparameters:\")\n",
        "        for key, value in best_trial.params.items():\n",
        "            print(f\"    {key}: {value}\")\n",
        "\n",
        "        # Update hyperparameters with best trial values\n",
        "        BATCH_SIZE = best_trial.params.get('batch_size', BATCH_SIZE)\n",
        "        LEARNING_RATE = best_trial.params.get('learning_rate', LEARNING_RATE)\n",
        "\n",
        "        if MODEL == 'MLP':\n",
        "            MLP_LAYERS = best_trial.params.get('num_layers', MLP_LAYERS)\n",
        "            MLP_HIDDEN_SIZE = best_trial.params.get('hidden_size', MLP_HIDDEN_SIZE)\n",
        "            MLP_DROPOUT = best_trial.params.get('dropout', MLP_DROPOUT)\n",
        "\n",
        "        elif MODEL == 'LSTM':\n",
        "            LSTM_LAYERS = best_trial.params.get('num_layers', LSTM_LAYERS)\n",
        "            LSTM_HIDDEN_SIZE = best_trial.params.get('hidden_size', LSTM_HIDDEN_SIZE)\n",
        "            LSTM_DROPOUT = best_trial.params.get('dropout', LSTM_DROPOUT)\n",
        "\n",
        "        elif MODEL == 'RNN':\n",
        "            RNN_LAYERS = best_trial.params.get('num_layers', RNN_LAYERS)\n",
        "            RNN_HIDDEN_SIZE = best_trial.params.get('hidden_size', RNN_HIDDEN_SIZE)\n",
        "            RNN_DROPOUT = best_trial.params.get('dropout', RNN_DROPOUT)\n",
        "\n",
        "        elif MODEL == 'GRU':\n",
        "            GRU_LAYERS = best_trial.params.get('num_layers', GRU_LAYERS)\n",
        "            GRU_HIDDEN_SIZE = best_trial.params.get('hidden_size', GRU_HIDDEN_SIZE)\n",
        "            GRU_DROPOUT = best_trial.params.get('dropout', GRU_DROPOUT)\n",
        "\n",
        "        elif MODEL == 'CNN1D':\n",
        "            CNN_LAYERS = best_trial.params.get('num_layers', CNN_LAYERS)\n",
        "            CNN_HIDDEN_SIZE = best_trial.params.get('hidden_size', CNN_HIDDEN_SIZE)\n",
        "            CNN_DROPOUT = best_trial.params.get('dropout', CNN_DROPOUT)\n",
        "\n",
        "        elif MODEL in ['Transformer', 'TransformerCLS']:\n",
        "            TRANSFORMER_D_MODEL = best_trial.params.get('d_model', TRANSFORMER_D_MODEL)\n",
        "            TRANSFORMER_NHEAD = best_trial.params.get('nhead', TRANSFORMER_NHEAD)\n",
        "            TRANSFORMER_LAYERS = best_trial.params.get('num_layers', TRANSFORMER_LAYERS)\n",
        "            TRANSFORMER_DIM_FEEDFORWARD = best_trial.params.get('dim_feedforward', TRANSFORMER_DIM_FEEDFORWARD)\n",
        "            TRANSFORMER_DROPOUT = best_trial.params.get('dropout', TRANSFORMER_DROPOUT)\n",
        "\n",
        "        print(f\"\\n✓ Hyperparameters updated with best trial values\")\n",
        "        print(f\"  Learning rate: {LEARNING_RATE:.6f}\")\n",
        "        print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "\n",
        "        # Save optimization results\n",
        "        optuna_results_path = os.path.join(OUTPUT_PATH, 'optuna_results.csv')\n",
        "        trials_df = study.trials_dataframe()\n",
        "        trials_df.to_csv(optuna_results_path, index=False)\n",
        "        print(f\"\\n✓ Saved Optuna results to: {optuna_results_path}\")\n",
        "\n",
        "        # Save best hyperparameters\n",
        "        best_params_path = os.path.join(OUTPUT_PATH, 'best_hyperparameters.txt')\n",
        "        with open(best_params_path, 'w') as f:\n",
        "            f.write(f\"Best Hyperparameters for {MODEL}\\n\")\n",
        "            f.write(\"=\"*60 + \"\\n\\n\")\n",
        "            f.write(f\"Validation Loss: {best_trial.value:.6f}\\n\\n\")\n",
        "            f.write(\"Parameters:\\n\")\n",
        "            for key, value in best_trial.params.items():\n",
        "                f.write(f\"  {key}: {value}\\n\")\n",
        "        print(f\"✓ Saved best hyperparameters to: {best_params_path}\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\n⚠ Hyperparameter optimization disabled (OPTIMIZE_HYPERPARAMETERS=False)\")\n",
        "\n",
        "\n",
        "    # ========================================================================\n",
        "    # FOLD-WISE TRAINING AND EVALUATION\n",
        "    # ========================================================================\n",
        "\n",
        "    fold_metrics = []\n",
        "    fold_smape_distributions = []\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    for fold_idx, fold_config in enumerate(folds):\n",
        "\n",
        "        # Fold Variables\n",
        "        fold_output_dir = os.path.join(OUTPUT_PATH, f\"fold_{fold_idx + 1}\")\n",
        "        os.makedirs(fold_output_dir, exist_ok=True)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"{fold_config['name'].upper()}: {fold_config['test_start']} to {fold_config['test_end']}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Filter training data up to train_end date\n",
        "        df_train = df_full[df_full['Date'] <= fold_config['train_end']].copy()\n",
        "\n",
        "        print(f\"\\nFiltered training data up to {fold_config['train_end']}\")\n",
        "        print(f\"  Training observations: {len(df_train):,}\")\n",
        "        print(f\"  Date range: {df_train['Date'].min()} to {df_train['Date'].max()}\")\n",
        "\n",
        "        # -----------------------------------------------------------------\n",
        "        # BASELINE MODEL: Moving Average\n",
        "        # -----------------------------------------------------------------\n",
        "        if MODEL == 'BASELINE':\n",
        "            print(\"\\n\" + \"-\"*60)\n",
        "            print(\"BASELINE MODEL: Calculating Moving Average Predictions\")\n",
        "            print(\"-\"*60)\n",
        "\n",
        "            # Get test period data\n",
        "            df_test_period = df_full[\n",
        "                (df_full['Date'] >= fold_config['test_start']) &\n",
        "                (df_full['Date'] <= fold_config['test_end'])\n",
        "            ].copy()\n",
        "\n",
        "            # Calculate moving average baseline for each time series\n",
        "            predictions_dict = {}\n",
        "            actuals_dict = {}\n",
        "            all_preds = []\n",
        "            all_acts = []\n",
        "\n",
        "            unique_ts_keys = df_test_period['ts_key'].unique()\n",
        "            print(f\"Generating baseline predictions for {len(unique_ts_keys)} time series...\")\n",
        "\n",
        "            for ts_key in unique_ts_keys:\n",
        "                # Get last SEQ_LENGTH values up to train_end for this time series\n",
        "                ts_train_data = df_train[df_train['ts_key'] == ts_key].sort_values('Date')\n",
        "\n",
        "                if len(ts_train_data) < SEQ_LENGTH + EMBARGO:\n",
        "                    # If not enough history, use all available data\n",
        "                    baseline_value = ts_train_data['Value'].mean()\n",
        "                else:\n",
        "                    # Use last SEQ_LENGTH values, skipping the EMBARGO period\n",
        "                    baseline_value = ts_train_data['Value'].iloc[-(SEQ_LENGTH + EMBARGO):-EMBARGO].mean()\n",
        "\n",
        "                # Get actual values for this time series in test period\n",
        "                ts_test_data = df_test_period[df_test_period['ts_key'] == ts_key].sort_values('Date')\n",
        "                actual_values = ts_test_data['Value'].values\n",
        "\n",
        "                # Predict the same baseline value for all test periods\n",
        "                predicted_values = np.full(len(actual_values), baseline_value)\n",
        "\n",
        "                # Round prediction to 1 decimal place\n",
        "                predicted_values = np.round(predicted_values, 1)\n",
        "\n",
        "                # Replance NaN prediction with zero\n",
        "                predicted_values = np.where(np.isnan(predicted_values), 0, predicted_values)\n",
        "\n",
        "                predictions_dict[ts_key] = predicted_values\n",
        "                actuals_dict[ts_key] = actual_values\n",
        "                all_preds.extend(predicted_values)\n",
        "                all_acts.extend(actual_values)\n",
        "\n",
        "            all_preds = np.array(all_preds)\n",
        "            all_acts = np.array(all_acts)\n",
        "\n",
        "            print(f\"✓ Generated {len(all_preds)} baseline predictions\")\n",
        "\n",
        "        else:\n",
        "            # -----------------------------------------------------------------\n",
        "            # STEP 1: Create datasets for model development (train/val split)\n",
        "            # -----------------------------------------------------------------\n",
        "            print(\"\\n\" + \"-\"*60)\n",
        "            print(\"STEP 1: Creating datasets for model development\")\n",
        "            print(\"-\"*60)\n",
        "\n",
        "            train_dataset = TimeSeriesDataset(\n",
        "                df_train,\n",
        "                feature_cols=features,\n",
        "                seq_length=SEQ_LENGTH,\n",
        "                embargo=EMBARGO,\n",
        "                train=True,\n",
        "                train_ratio=TRAIN_RATIO\n",
        "            )\n",
        "\n",
        "            print(f\"Training samples: {len(train_dataset):,}\")\n",
        "\n",
        "            # Save scalers and metadata\n",
        "            scaler_X = train_dataset.scaler_X\n",
        "            scaler_y = train_dataset.scaler_y\n",
        "            n_ts_keys = train_dataset.n_ts_keys\n",
        "            ts_key_to_idx = train_dataset.ts_key_to_idx\n",
        "\n",
        "            test_dataset = TimeSeriesDataset(\n",
        "                df_train,\n",
        "                feature_cols=features,\n",
        "                seq_length=SEQ_LENGTH,\n",
        "                train=False,\n",
        "                embargo=EMBARGO,\n",
        "                train_ratio=TRAIN_RATIO,\n",
        "                scaler_X=scaler_X,\n",
        "                scaler_y=scaler_y\n",
        "            )\n",
        "\n",
        "            print(f\"Validation samples: {len(test_dataset):,}\")\n",
        "\n",
        "        # -----------------------------------------------------------------\n",
        "        # STEP 2: Initialize and train model\n",
        "        # -----------------------------------------------------------------\n",
        "            print(\"\\n\" + \"-\"*60)\n",
        "            print(f\"STEP 2: Training {MODEL} model\")\n",
        "            print(\"-\"*60)\n",
        "\n",
        "            INPUT_SIZE = train_dataset.X.shape[2]\n",
        "\n",
        "            if MODEL == 'LSTM':\n",
        "                model = LSTMForecaster(\n",
        "                    input_size=INPUT_SIZE,\n",
        "                    hidden_size=LSTM_HIDDEN_SIZE,\n",
        "                    num_layers=LSTM_LAYERS,\n",
        "                    dropout=LSTM_DROPOUT\n",
        "                ).to(device)\n",
        "            elif MODEL == 'RNN':\n",
        "                model = RNNForecaster(\n",
        "                    input_size=INPUT_SIZE,\n",
        "                    hidden_size=RNN_HIDDEN_SIZE,\n",
        "                    num_layers=RNN_LAYERS,\n",
        "                    dropout=RNN_DROPOUT\n",
        "                ).to(device)\n",
        "            elif MODEL == 'GRU':\n",
        "                model = GRUForecaster(\n",
        "                    input_size=INPUT_SIZE,\n",
        "                    hidden_size=GRU_HIDDEN_SIZE,\n",
        "                    num_layers=GRU_LAYERS,\n",
        "                    dropout=GRU_DROPOUT\n",
        "                ).to(device)\n",
        "            elif MODEL == 'CNN1D':\n",
        "                model = CNN1DForecaster(\n",
        "                    input_size=INPUT_SIZE,\n",
        "                    hidden_size=CNN_HIDDEN_SIZE,\n",
        "                    num_layers=CNN_LAYERS,\n",
        "                    dropout=CNN_DROPOUT\n",
        "                ).to(device)\n",
        "            elif MODEL == 'MLP':\n",
        "                model = MLPForecaster(\n",
        "                    input_size=INPUT_SIZE,\n",
        "                    seq_length=SEQ_LENGTH,\n",
        "                    hidden_size=MLP_HIDDEN_SIZE,\n",
        "                    num_layers=MLP_LAYERS,\n",
        "                    dropout=MLP_DROPOUT\n",
        "                ).to(device)\n",
        "            elif MODEL == 'Transformer':\n",
        "                model = TransformerForecaster(\n",
        "                    input_size=INPUT_SIZE,\n",
        "                    d_model=TRANSFORMER_D_MODEL,\n",
        "                    nhead=TRANSFORMER_NHEAD,\n",
        "                    num_layers=TRANSFORMER_LAYERS,\n",
        "                    dim_feedforward=TRANSFORMER_DIM_FEEDFORWARD,\n",
        "                    dropout=TRANSFORMER_DROPOUT\n",
        "                ).to(device)\n",
        "            elif MODEL == 'TransformerCLS':\n",
        "                model = TransformerForecasterCLS(\n",
        "                    input_size=INPUT_SIZE,\n",
        "                    d_model=TRANSFORMER_D_MODEL,\n",
        "                    nhead=TRANSFORMER_NHEAD,\n",
        "                    num_layers=TRANSFORMER_LAYERS,\n",
        "                    dim_feedforward=TRANSFORMER_DIM_FEEDFORWARD,\n",
        "                    dropout=TRANSFORMER_DROPOUT\n",
        "                ).to(device)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported MODEL type: {MODEL}\")\n",
        "\n",
        "            print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "            val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "            criterion = nn.MSELoss()\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "            best_val_loss = float('inf')\n",
        "            train_losses = []\n",
        "            val_losses = []\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            for epoch in range(EPOCHS):\n",
        "                train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "                val_loss = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "\n",
        "                scheduler.step(val_loss)\n",
        "\n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    best_model_state = model.state_dict().copy()\n",
        "\n",
        "                if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "                    elapsed = time.time() - start_time\n",
        "                    print(f\"Epoch [{epoch+1:3d}/{EPOCHS}] | Train: {train_loss:.6f} | Val: {val_loss:.6f} | Time: {elapsed:.1f}s\")\n",
        "\n",
        "            # Load best model\n",
        "            model.load_state_dict(best_model_state)\n",
        "\n",
        "            training_time = time.time() - start_time  # in seconds\n",
        "\n",
        "            TOTAL_TRAINING_TIME_FOLDS[fold_config['name']] = training_time\n",
        "\n",
        "            print(f\"\\nTraining completed in {training_time:.1f} seconds\")\n",
        "            print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
        "\n",
        "\n",
        "            # -----------------------------------------------------------------\n",
        "            # STEP 3: Out-of-sample predictions on test period\n",
        "            # -----------------------------------------------------------------\n",
        "\n",
        "            # Get test period data\n",
        "            df_test_period = df_full[\n",
        "                (df_full['Date'] >= fold_config['test_start']) &\n",
        "                (df_full['Date'] <= fold_config['test_end'])\n",
        "            ].copy()\n",
        "\n",
        "\n",
        "            predictions_dict, actuals_dict, all_preds, all_acts = generate_out_of_sample_predictions(model=model,\n",
        "                df_test_period=df_test_period,\n",
        "                df_full=df_full,\n",
        "                fold_config=fold_config,\n",
        "                features=features,\n",
        "                scaler_X=scaler_X,\n",
        "                scaler_y=scaler_y,\n",
        "                ts_key_to_idx=ts_key_to_idx,\n",
        "                n_ts_keys=n_ts_keys,\n",
        "                seq_length=SEQ_LENGTH,\n",
        "                embargo=EMBARGO,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "\n",
        "        # -----------------------------------------------------------------\n",
        "        # Create predictions DataFrame and save as CSV\n",
        "        # -----------------------------------------------------------------\n",
        "        predictions_data = []\n",
        "        for ts_key, preds in predictions_dict.items():\n",
        "            acts = actuals_dict[ts_key]\n",
        "            # Get the dates for this time series in the test period\n",
        "            ts_group = df_test_period[df_test_period['ts_key'] == ts_key].sort_values('Date')\n",
        "            dates = ts_group['Date'].values[:len(preds)]\n",
        "\n",
        "            for i, (date, pred, actual) in enumerate(zip(dates, preds, acts)):\n",
        "                predictions_data.append({\n",
        "                    'ts_key': ts_key,\n",
        "                    'Date': pd.to_datetime(date),\n",
        "                    'pred': pred,\n",
        "                    'true': actual\n",
        "                })\n",
        "\n",
        "        predictions_df = pd.DataFrame(predictions_data)\n",
        "        predictions_csv_path = os.path.join(fold_output_dir, 'predictions.csv')\n",
        "        predictions_df.to_csv(predictions_csv_path, index=False)\n",
        "        print(f\"✓ Saved predictions to: predictions.csv ({len(predictions_df)} rows)\")\n",
        "\n",
        "        # -----------------------------------------------------------------\n",
        "        # Calculate SMAPE distribution\n",
        "        # -----------------------------------------------------------------\n",
        "        smape_distribution_df = calculate_smape_distribution(predictions_dict, actuals_dict)\n",
        "\n",
        "        # Calculate category counts and percentages\n",
        "        category_counts = smape_distribution_df['category'].value_counts()\n",
        "        total_series = len(smape_distribution_df)\n",
        "\n",
        "        # Define category order\n",
        "        category_order = ['<10%', '10-20%', '20-30%', '30-40%', '>40%']\n",
        "        category_distribution = {}\n",
        "\n",
        "        for cat in category_order:\n",
        "            count = category_counts.get(cat, 0)\n",
        "            percentage = (count / total_series * 100) if total_series > 0 else 0\n",
        "            category_distribution[cat] = {\n",
        "                'count': count,\n",
        "                'percentage': percentage\n",
        "            }\n",
        "\n",
        "        fold_smape_distributions.append({\n",
        "            'fold': fold_config['name'],\n",
        "            **{f'{cat}_count': category_distribution[cat]['count'] for cat in category_order},\n",
        "            **{f'{cat}_pct': category_distribution[cat]['percentage'] for cat in category_order}\n",
        "        })\n",
        "\n",
        "        # Save SMAPE distribution per time series\n",
        "        smape_distribution_df.to_csv(os.path.join(fold_output_dir, 'smape_per_ts.csv'), index=False)\n",
        "\n",
        "        print(f\"\\nSMAPE Distribution for {fold_config['name']}:\")\n",
        "        print(f\"  Time series evaluated: {total_series}\")\n",
        "        for cat in category_order:\n",
        "            count = category_distribution[cat]['count']\n",
        "            pct = category_distribution[cat]['percentage']\n",
        "            print(f\"  {cat:>10}: {count:4d} series ({pct:5.1f}%)\")\n",
        "\n",
        "        # -----------------------------------------------------------------\n",
        "        # STEP 4: Calculate metrics\n",
        "        # -----------------------------------------------------------------\n",
        "        print(\"\\n\" + \"-\"*60)\n",
        "        print(\"STEP 4: Evaluation Metrics\")\n",
        "        print(\"-\"*60)\n",
        "\n",
        "        mse = mean_squared_error(all_acts, all_preds)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(all_acts, all_preds)\n",
        "        r2 = r2_score(all_acts, all_preds)\n",
        "        smape_score = smape(all_acts, all_preds)\n",
        "\n",
        "        fold_metrics.append({\n",
        "            'fold': fold_config['name'],\n",
        "            'mse': mse,\n",
        "            'rmse': rmse,\n",
        "            'mae': mae,\n",
        "            'r2': r2,\n",
        "            'smape': smape_score\n",
        "        })\n",
        "\n",
        "        print(f\"\\n{fold_config['name']} Out-of-Sample Metrics:\")\n",
        "        print(f\"  MSE:   {mse:.2f}\")\n",
        "        print(f\"  RMSE:  {rmse:.2f}\")\n",
        "        print(f\"  MAE:   {mae:.2f}\")\n",
        "        print(f\"  R²:    {r2:.4f}\")\n",
        "        print(f\"  SMAPE: {smape_score:.2f}%\")\n",
        "\n",
        "        # Save model checkpoint\n",
        "        if MODEL not in ['BASELINE']:\n",
        "            torch.save({\n",
        "                'model_state_dict': best_model_state,\n",
        "                'input_size': INPUT_SIZE,\n",
        "                'hidden_size': 64,\n",
        "                'num_layers': 2,\n",
        "                'dropout': 0.2,\n",
        "                'scaler_X': scaler_X,\n",
        "                'scaler_y': scaler_y,\n",
        "                'n_ts_keys': n_ts_keys,\n",
        "                'ts_key_to_idx': ts_key_to_idx,\n",
        "                'seq_length': SEQ_LENGTH,\n",
        "                'fold_config': fold_config,\n",
        "                'metrics': fold_metrics[-1]\n",
        "            }, os.path.join(fold_output_dir, 'model_checkpoint.pth'))\n",
        "\n",
        "        # Visualization\n",
        "        if MODEL not in ['BASELINE']:\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "            # Training history\n",
        "            axes[0].plot(train_losses, label='Training Loss', linewidth=2, color='#2E86AB')\n",
        "            axes[0].plot(val_losses, label='Validation Loss', linewidth=2, color='#A23B72')\n",
        "            axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "            axes[0].set_ylabel('Loss (MSE)', fontsize=12)\n",
        "            axes[0].set_title(f'{fold_config[\"name\"]} - Training History', fontsize=13, fontweight='bold')\n",
        "            axes[0].legend(fontsize=10)\n",
        "            axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "            # Predictions vs Actuals\n",
        "            axes[1].scatter(all_acts, all_preds, alpha=0.3, s=10, color='#2E86AB')\n",
        "            axes[1].plot([all_acts.min(), all_acts.max()],\n",
        "                    [all_acts.min(), all_acts.max()],\n",
        "                    'r--', linewidth=2, label='Perfect Prediction')\n",
        "            axes[1].set_xlabel('Actual Values', fontsize=12)\n",
        "            axes[1].set_ylabel('Predicted Values', fontsize=12)\n",
        "            axes[1].set_title(f'{fold_config[\"name\"]} - Predictions (R²={r2:.4f}, SMAPE={smape_score:.2f}%)',\n",
        "                    fontsize=13, fontweight='bold')\n",
        "            axes[1].legend(fontsize=10)\n",
        "            axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(fold_output_dir, 'fold_results.png'), dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "        # Predictions vs Actuals (for all models including BASELINE)\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "        ax.scatter(all_acts, all_preds, alpha=0.3, s=10, color='#2E86AB')\n",
        "        ax.plot([all_acts.min(), all_acts.max()],\n",
        "            [all_acts.min(), all_acts.max()],\n",
        "            'r--', linewidth=2, label='Perfect Prediction')\n",
        "        ax.set_xlabel('Actual Values', fontsize=12)\n",
        "        ax.set_ylabel('Predicted Values', fontsize=12)\n",
        "        ax.set_title(f'{fold_config[\"name\"]} - Predictions (R²={r2:.4f}, SMAPE={smape_score:.2f}%)',\n",
        "                 fontsize=13, fontweight='bold')\n",
        "        ax.legend(fontsize=10)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(fold_output_dir, 'predictions_vs_actuals.png'), dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"\\n✓ Saved fold results to: {fold_output_dir}\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # SMAPE DISTRIBUTION ANALYSIS\n",
        "    # ========================================================================\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SMAPE DISTRIBUTION ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    smape_dist_df = pd.DataFrame(fold_smape_distributions)\n",
        "\n",
        "    print(\"\\nPer-Fold SMAPE Distribution:\")\n",
        "    category_order = ['<10%', '10-20%', '20-30%', '30-40%', '>40%']\n",
        "\n",
        "    for _, row in smape_dist_df.iterrows():\n",
        "        print(f\"\\n{row['fold']}:\")\n",
        "        for cat in category_order:\n",
        "            count = row[f'{cat}_count']\n",
        "            pct = row[f'{cat}_pct']\n",
        "            print(f\"  {cat:>10}: {count:4.0f} series ({pct:5.1f}%)\")\n",
        "\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"AVERAGE SMAPE DISTRIBUTION ACROSS FOLDS:\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    for cat in category_order:\n",
        "        avg_count = smape_dist_df[f'{cat}_count'].mean()\n",
        "        avg_pct = smape_dist_df[f'{cat}_pct'].mean()\n",
        "        std_pct = smape_dist_df[f'{cat}_pct'].std()\n",
        "        print(f\"  {cat:>10}: {avg_pct:5.1f}% ± {std_pct:4.1f}% ({avg_count:.0f} series avg)\")\n",
        "\n",
        "    # Save SMAPE distribution summary\n",
        "    smape_dist_df.to_csv(os.path.join(OUTPUT_PATH, 'smape_distribution.csv'), index=False)\n",
        "\n",
        "    print(f\"\\n✓ Saved metrics to: {OUTPUT_PATH}/fold_metrics.csv\")\n",
        "    print(f\"✓ Saved summary to: {OUTPUT_PATH}/summary_metrics.csv\")\n",
        "    print(f\"✓ Saved SMAPE distribution to: {OUTPUT_PATH}/smape_distribution.csv\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # FINAL RESULTS: Average metrics across all folds\n",
        "    # ========================================================================\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FINAL RESULTS: AVERAGE METRICS ACROSS ALL FOLDS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    metrics_df = pd.DataFrame(fold_metrics)\n",
        "\n",
        "    print(\"\\nPer-Fold Metrics:\")\n",
        "    print(metrics_df.to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"AVERAGE PERFORMANCE:\")\n",
        "    print(\"-\"*60)\n",
        "    avg_metrics = metrics_df[['mse', 'rmse', 'mae', 'r2', 'smape']].mean()\n",
        "    std_metrics = metrics_df[['mse', 'rmse', 'mae', 'r2', 'smape']].std()\n",
        "\n",
        "    print(f\"  MSE:   {avg_metrics['mse']:.2f} ± {std_metrics['mse']:.2f}\")\n",
        "    print(f\"  RMSE:  {avg_metrics['rmse']:.2f} ± {std_metrics['rmse']:.2f}\")\n",
        "    print(f\"  MAE:   {avg_metrics['mae']:.2f} ± {std_metrics['mae']:.2f}\")\n",
        "    print(f\"  R²:    {avg_metrics['r2']:.4f} ± {std_metrics['r2']:.4f}\")\n",
        "    print(f\"  SMAPE: {avg_metrics['smape']:.2f}% ± {std_metrics['smape']:.2f}%\")\n",
        "\n",
        "    # Save metrics to CSV\n",
        "    metrics_df.to_csv(os.path.join(OUTPUT_PATH, 'fold_metrics.csv'), index=False)\n",
        "\n",
        "    # Save summary\n",
        "    summary_dict = {\n",
        "        'metric': ['MSE', 'RMSE', 'MAE', 'R²', 'SMAPE'],\n",
        "        'mean': [avg_metrics['mse'], avg_metrics['rmse'], avg_metrics['mae'],\n",
        "                 avg_metrics['r2'], avg_metrics['smape']],\n",
        "        'std': [std_metrics['mse'], std_metrics['rmse'], std_metrics['mae'],\n",
        "                std_metrics['r2'], std_metrics['smape']]\n",
        "    }\n",
        "    summary_df = pd.DataFrame(summary_dict)\n",
        "    summary_df.to_csv(os.path.join(OUTPUT_PATH, 'summary_metrics.csv'), index=False)\n",
        "\n",
        "    TOTAL_SCRIPT_RUNTIME = (time.time() - SCRIPT_START_TIME) / 60.0  # in minutes\n",
        "    print(f\"\\nTotal script runtime: {TOTAL_SCRIPT_RUNTIME:.1f} minutes\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # SAVE COMPREHENSIVE RESULTS TO TXT FILE\n",
        "    # ========================================================================\n",
        "\n",
        "    results_txt_path = os.path.join(OUTPUT_PATH, 'final_results_summary.txt')\n",
        "\n",
        "    with open(results_txt_path, 'w') as f:\n",
        "        f.write(\"=\"*80 + \"\\n\")\n",
        "        f.write(f\"{MODEL} MODEL - FINAL EVALUATION RESULTS\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "        f.write(f\"Model: {MODEL}\\n\")\n",
        "        if best_trial is not None:\n",
        "            f.write(\"Best Hyperparameters:\\n\")\n",
        "            for key, value in best_trial.params.items():\n",
        "                f.write(f\"    {key}: {value}\\n\")\n",
        "\n",
        "        # Total optimization time\n",
        "        if OPTIMIZE_HYPERPARAMETERS and best_trial is not None:\n",
        "            f.write(f\"\\nTotal Hyperparameter Optimization Time: {TOTAL_OPTIMIZATION_TIME:.1f} minutes\\n\")\n",
        "\n",
        "        # Training time per fold\n",
        "        f.write(\"\\nTraining Time per Fold:\\n\")\n",
        "        for fold_name, training_time in TOTAL_TRAINING_TIME_FOLDS.items():\n",
        "            f.write(f\"  {fold_name}: {training_time:.1f} seconds\\n\")\n",
        "\n",
        "        f.write(f\"\\nTotal Script Runtime: {TOTAL_SCRIPT_RUNTIME:.1f} minutes\\n\")\n",
        "\n",
        "\n",
        "        f.write(\"\\n\" + \"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "        # 1. FINAL RESULTS: AVERAGE METRICS ACROSS ALL FOLDS\n",
        "        f.write(\"=\"*80 + \"\\n\")\n",
        "        f.write(\"1. FINAL RESULTS: AVERAGE METRICS ACROSS ALL FOLDS\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "\n",
        "        f.write(\"Per-Fold Metrics:\\n\")\n",
        "        f.write(metrics_df.to_string(index=False) + \"\\n\\n\")\n",
        "\n",
        "        # 4. AVERAGE PERFORMANCE\n",
        "        f.write(\"-\"*60 + \"\\n\")\n",
        "        f.write(\"4. AVERAGE PERFORMANCE:\\n\")\n",
        "        f.write(\"-\"*60 + \"\\n\")\n",
        "        f.write(f\"  MSE:   {avg_metrics['mse']:.2f} ± {std_metrics['mse']:.2f}\\n\")\n",
        "        f.write(f\"  RMSE:  {avg_metrics['rmse']:.2f} ± {std_metrics['rmse']:.2f}\\n\")\n",
        "        f.write(f\"  MAE:   {avg_metrics['mae']:.2f} ± {std_metrics['mae']:.2f}\\n\")\n",
        "        f.write(f\"  R²:    {avg_metrics['r2']:.4f} ± {std_metrics['r2']:.4f}\\n\")\n",
        "        f.write(f\"  SMAPE: {avg_metrics['smape']:.2f}% ± {std_metrics['smape']:.2f}%\\n\\n\")\n",
        "\n",
        "        # 2. SMAPE DISTRIBUTION ANALYSIS per fold\n",
        "        f.write(\"=\"*80 + \"\\n\")\n",
        "        f.write(\"2. SMAPE DISTRIBUTION ANALYSIS PER FOLD\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "        for _, row in smape_dist_df.iterrows():\n",
        "            f.write(f\"{row['fold']}:\\n\")\n",
        "            for cat in category_order:\n",
        "                count = row[f'{cat}_count']\n",
        "                pct = row[f'{cat}_pct']\n",
        "                f.write(f\"  {cat:>10}: {count:4.0f} series ({pct:5.1f}%)\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "        # 3. AVERAGE SMAPE DISTRIBUTION ACROSS FOLDS\n",
        "        f.write(\"-\"*60 + \"\\n\")\n",
        "        f.write(\"3. AVERAGE SMAPE DISTRIBUTION ACROSS FOLDS:\\n\")\n",
        "        f.write(\"-\"*60 + \"\\n\")\n",
        "\n",
        "        for cat in category_order:\n",
        "            avg_count = smape_dist_df[f'{cat}_count'].mean()\n",
        "            avg_pct = smape_dist_df[f'{cat}_pct'].mean()\n",
        "            std_pct = smape_dist_df[f'{cat}_pct'].std()\n",
        "            f.write(f\"  {cat:>10}: {avg_pct:5.1f}% ± {std_pct:4.1f}% ({avg_count:.0f} series avg)\\n\")\n",
        "\n",
        "        f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "        f.write(\"END OF REPORT\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    print(f\"\\n✓ Saved metrics to: {OUTPUT_PATH}/fold_metrics.csv\")\n",
        "    print(f\"✓ Saved summary to: {OUTPUT_PATH}/summary_metrics.csv\")\n",
        "    print(f\"✓ Saved final results summary to: {OUTPUT_PATH}/final_results_summary.txt\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "afb46504a33b46db8069a824f4084232",
            "48592bf461c547f0ab8468f626c66c54",
            "e36e99aa55c143149bd58bc8de00a002",
            "31e5af6c515a4f3f8a4bdba1b1ecb3da",
            "8b7721087c0f43379756427d2ccc3ec9",
            "052b70e0b6f34327bd68474511600f4a",
            "42c259d6e7fa4d3e846c257a9ea6e040",
            "bd2d6054570b4b188894d243c5cbb3b7",
            "7ae9c9bf2caa4a4db82af713a93e42c4",
            "88ea1fb63f684e4d88bfa999762df6aa",
            "7169efedb6714a9aae167ea3cdc105b8"
          ]
        },
        "id": "H0BwG9MmziCt",
        "outputId": "04fe5d39-2f75-42e7-b9cb-c213feedca72",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ CUDA device is available\n",
            "Device: cuda\n",
            "================================================================================\n",
            "DATA OVERVIEW\n",
            "================================================================================\n",
            "Original data shape: (107922, 29)\n",
            "Unique time series: 1502\n",
            "Date range: 2018-01-31 00:00:00 to 2025-09-30 00:00:00\n",
            "Total observations: 107,922\n",
            "\n",
            "Original DataFrame:\n",
            "  NaN values: 0\n",
            "  Inf values: 0\n",
            "  Value range: [0.00, 22335.00]\n",
            "\n",
            "Zero values in 'Value' column: 20818\n",
            "Values < 0.01: 20818\n",
            "\n",
            "================================================================================\n",
            "FOLD CONFIGURATION\n",
            "================================================================================\n",
            "\n",
            "Fold 1:\n",
            "  Training data: up to 2024-09-30\n",
            "  Test period: 2024-10-01 to 2024-12-31\n",
            "\n",
            "Fold 2:\n",
            "  Training data: up to 2024-12-31\n",
            "  Test period: 2025-01-01 to 2025-03-31\n",
            "\n",
            "Fold 3:\n",
            "  Training data: up to 2025-06-30\n",
            "  Test period: 2025-07-01 to 2025-09-30\n",
            "\n",
            "================================================================================\n",
            "HYPERPARAMETER OPTIMIZATION WITH OPTUNA\n",
            "================================================================================\n",
            "Model: RNN\n",
            "Trials: 3\n",
            "Timeout: 900s\n",
            "\n",
            "Using Fold 1 for optimization\n",
            "  Training observations: 90,494\n",
            "Creating dataset with 1478 time series...\n",
            "Additional features: 2\n",
            "Sequence length: 6, Embargo: 1\n",
            "Created 80408 samples with feature dimension: 1483\n",
            "  - Value: 1\n",
            "  - Additional features: 2\n",
            "  - Temporal (year, month): 2\n",
            "  - One-hot ts_key: 1478\n",
            "Creating dataset with 1478 time series...\n",
            "Additional features: 2\n",
            "Sequence length: 6, Embargo: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-12-25 19:57:51,593] A new study created in memory with name: RNN_optimization\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 80408 samples with feature dimension: 1483\n",
            "  - Value: 1\n",
            "  - Additional features: 2\n",
            "  - Temporal (year, month): 2\n",
            "  - One-hot ts_key: 1478\n",
            "\n",
            "Optimization dataset:\n",
            "  Training samples: 64,326\n",
            "  Validation samples: 16,082\n",
            "  Input size: 1483\n",
            "\n",
            "Starting optimization...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afb46504a33b46db8069a824f4084232"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[I 2025-12-25 19:58:21,800] Trial 0 finished with value: 1.1160497942362868 and parameters: {'learning_rate': 0.0008537722282739753, 'batch_size': 128, 'num_layers': 2, 'hidden_size': 128, 'dropout': 0.4898213195128033}. Best is trial 0 with value: 1.1160497942362868.\n",
            "[I 2025-12-25 20:01:17,015] Trial 1 finished with value: 1.1911415764265814 and parameters: {'learning_rate': 0.00018361420858514188, 'batch_size': 32, 'num_layers': 3, 'hidden_size': 32, 'dropout': 0.2903072283700481}. Best is trial 0 with value: 1.1160497942362868.\n",
            "[I 2025-12-25 20:01:47,595] Trial 2 finished with value: 1.2309573691759614 and parameters: {'learning_rate': 0.0011128672025514763, 'batch_size': 128, 'num_layers': 2, 'hidden_size': 256, 'dropout': 0.33339805609594136}. Best is trial 0 with value: 1.1160497942362868.\n",
            "\n",
            "Optimization completed in 3.9 minutes\n",
            "\n",
            "================================================================================\n",
            "OPTIMIZATION RESULTS\n",
            "================================================================================\n",
            "Number of finished trials: 3\n",
            "\n",
            "Best trial:\n",
            "  Value (Validation Loss): 1.116050\n",
            "  Hyperparameters:\n",
            "    learning_rate: 0.0008537722282739753\n",
            "    batch_size: 128\n",
            "    num_layers: 2\n",
            "    hidden_size: 128\n",
            "    dropout: 0.4898213195128033\n",
            "\n",
            "✓ Hyperparameters updated with best trial values\n",
            "  Learning rate: 0.000854\n",
            "  Batch size: 128\n",
            "\n",
            "✓ Saved Optuna results to: /content/drive/MyDrive/HDM/models/rnn_uni_one_hot/optuna_results.csv\n",
            "✓ Saved best hyperparameters to: /content/drive/MyDrive/HDM/models/rnn_uni_one_hot/best_hyperparameters.txt\n",
            "\n",
            "================================================================================\n",
            "FOLD 1: 2024-10-01 to 2024-12-31\n",
            "================================================================================\n",
            "\n",
            "Filtered training data up to 2024-09-30\n",
            "  Training observations: 90,494\n",
            "  Date range: 2018-01-31 00:00:00 to 2024-09-30 00:00:00\n",
            "\n",
            "------------------------------------------------------------\n",
            "STEP 1: Creating datasets for model development\n",
            "------------------------------------------------------------\n",
            "Creating dataset with 1478 time series...\n",
            "Additional features: 2\n",
            "Sequence length: 6, Embargo: 1\n",
            "Created 80408 samples with feature dimension: 1483\n",
            "  - Value: 1\n",
            "  - Additional features: 2\n",
            "  - Temporal (year, month): 2\n",
            "  - One-hot ts_key: 1478\n",
            "Training samples: 64,326\n",
            "Creating dataset with 1478 time series...\n",
            "Additional features: 2\n",
            "Sequence length: 6, Embargo: 1\n",
            "Created 80408 samples with feature dimension: 1483\n",
            "  - Value: 1\n",
            "  - Additional features: 2\n",
            "  - Temporal (year, month): 2\n",
            "  - One-hot ts_key: 1478\n",
            "Validation samples: 16,082\n",
            "\n",
            "------------------------------------------------------------\n",
            "STEP 2: Training RNN model\n",
            "------------------------------------------------------------\n",
            "Model parameters: 239,617\n",
            "Epoch [  1/25] | Train: 0.188155 | Val: 1.295428 | Time: 3.8s\n",
            "Epoch [ 10/25] | Train: 0.159540 | Val: 1.444867 | Time: 37.5s\n",
            "Epoch [ 20/25] | Train: 0.145948 | Val: 1.177976 | Time: 75.5s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v1kBbeHJXInU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}