{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Approach in Pytorch\n",
    "\n",
    "This notebook presents a standardized modeling framework in PyTorch for neural network evaluation. It provides reusable functions and methods to test various neural network architectures, enabling consistent performance comparison across different models.\n",
    "\n",
    "### Objective\n",
    "\n",
    "Our goal is to generate **3-month-ahead forecasts** for all time series in the dataset. To ensure robust model evaluation and prevent overfitting, we validate performance across three distinct test periods covering different seasonal patterns:\n",
    "\n",
    "- **Test Period 1:** October – December 2024\n",
    "- **Test Period 2:** January – March 2025\n",
    "- **Test Period 3:** July – September 2025\n",
    "\n",
    "Each model is trained three times using data prior to each respective test period.\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "For each test period, the following metrics are computed:\n",
    "\n",
    "- **MSE**: Mean Squared Error\n",
    "- **RMSE**: Root Mean Squared Error\n",
    "- **MAE**: Mean Absolute Error\n",
    "- **R²**: Coefficient of Determination\n",
    "- **SMAPE**: Symmetric Mean Absolute Percentage Error\n",
    "\n",
    "SMAPE serves as our primary business-relevance metric, with forecast quality assessed according to the following scale:\n",
    "\n",
    "| SMAPE Range | Quality Assessment |\n",
    "|-------------|-------------------|\n",
    "| <10% | Excellent |\n",
    "| 10-20% | Good |\n",
    "| 20-30% | Average |\n",
    "| 30-40% | Poor |\n",
    "| >40% | Unacceptable |\n",
    "\n",
    "### Output Format\n",
    "\n",
    "After each test period prediction, a `prediction.csv` file is generated containing predicted and actual values for all time series and timestamps:\n",
    "\n",
    "| ts_key | Date | pred | true |\n",
    "|--------|------|------|------|\n",
    "| BMW_1ER_Total | 2025-07-31 | 1425.0 | 2385.0 |\n",
    "| BMW_1ER_Total | 2025-08-31 | 1420.0 | 2124.0 |\n",
    "| BMW_1ER_Total | 2025-09-30 | 1350.0 | 2777.0 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeSeriesDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to correctly run our models in PyTorch, we need to define our dataset. We will create a `TimeSeriesDataset` class based on PyTorch's `Dataset` class, which generates many-to-one sequences for our neural networks. [Ref](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n",
    "**Key Features**\n",
    "\n",
    "- **Multivariate Time Series Support**: Handles multiple time series simultaneously with one-hot encoding for series identification\n",
    "- **Sliding Window Approach**: Creates sequences of fixed length (`seq_length`) to capture temporal patterns\n",
    "- **Embargo Period**: Introduces a gap between the last observation and the prediction target to prevent overfitting due to autocorrelation\n",
    "- **Memory Efficient**: Processes data without pivoting the entire dataframe\n",
    "\n",
    "**Feature Engineering**\n",
    "\n",
    "Each sample consists of the following features at each timestep:\n",
    "1. **Value**: The target variable from previous timesteps\n",
    "2. **Additional Features**: Economic indicators or other external variables (optional)\n",
    "3. **Temporal Features**: Year and month to capture seasonality\n",
    "4. **One-Hot Encoded ts_key**: Identifies which time series the sample belongs to\n",
    "\n",
    "**Embargo Mechanism**\n",
    "\n",
    "The embargo period creates a realistic forecasting scenario by skipping immediate past observations:\n",
    "\n",
    "**Example with `seq_length=3` and `embargo=1`:**\n",
    "- **Input (X)**: [t-4, t-3, t-2] \n",
    "- **Target (Y)**: t (skipping t-1)\n",
    "\n",
    "This prevents the model from learning shortcuts based on high autocorrelation in adjacent timesteps.\n",
    "\n",
    "**Data Scaling**\n",
    "- Continuous features (Value, additional features, year, month) are standardized using `StandardScaler`\n",
    "- One-hot encoded features remain unscaled to preserve their binary nature\n",
    "- Train and test sets use the same scaling parameters (fitted on training data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from neuralts.globals import FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Fold 1',\n",
       "  'train_end': '2024-09-30',\n",
       "  'test_start': '2024-10-01',\n",
       "  'test_end': '2024-12-31'},\n",
       " {'name': 'Fold 2',\n",
       "  'train_end': '2024-12-31',\n",
       "  'test_start': '2025-01-01',\n",
       "  'test_end': '2025-03-31'},\n",
       " {'name': 'Fold 3',\n",
       "  'train_end': '2025-06-30',\n",
       "  'test_start': '2025-07-01',\n",
       "  'test_end': '2025-09-30'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The test periods we will use\n",
    "FOLDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for MULTIVARIATE time series with sliding window approach and embargo period.\n",
    "    Each sample: [features(t-seq_length-embargo), ..., features(t-1-embargo)] -> target(t)\n",
    "    Features include: Value + additional features + year + month + one-hot encoded ts_key\n",
    "    Memory efficient: doesn't pivot the entire dataframe\n",
    "    \n",
    "    Embargo period creates a gap between last observation and prediction to avoid overfitting\n",
    "    due to autocorrelation. E.g., with seq_length=3 and embargo=1:\n",
    "        X = [t-4, t-3, t-2] -> Y = t (skipping t-1)\n",
    "    \"\"\"\n",
    "    def __init__(self, df, feature_cols=None, seq_length=6, embargo=1, train=True, train_ratio=0.8, scaler_X=None, scaler_y=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame with columns [Date, ts_key, Value, ...additional features]\n",
    "            feature_cols: List of additional feature column names (e.g., economic indicators)\n",
    "                         If None, will auto-detect (all columns except Date, ts_key, Value)\n",
    "            seq_length: Lookback window (number of timesteps)\n",
    "            embargo: Number of months to skip between last observation and prediction target\n",
    "            train: If True, create training set, else test set\n",
    "            train_ratio: Train/test split ratio\n",
    "            scaler_X: Pre-fitted StandardScaler for features (used for test set)\n",
    "            scaler_y: Pre-fitted StandardScaler for target (used for test set)\n",
    "        \"\"\"\n",
    "        self.seq_length = seq_length\n",
    "        self.embargo = embargo\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.ts_keys_list = []\n",
    "        \n",
    "        # Auto-detect feature columns if not provided\n",
    "        if feature_cols is None:\n",
    "            self.feature_cols = [col for col in df.columns \n",
    "                                if col not in ['Date', 'ts_key', 'Value']]\n",
    "        else:\n",
    "            self.feature_cols = feature_cols\n",
    "        \n",
    "        self.n_features_additional = len(self.feature_cols)\n",
    "        \n",
    "        # Create one-hot encoding mapping for ts_keys\n",
    "        unique_ts_keys = sorted(df['ts_key'].unique())\n",
    "        self.ts_key_to_idx = {key: idx for idx, key in enumerate(unique_ts_keys)}\n",
    "        self.n_ts_keys = len(unique_ts_keys)\n",
    "        \n",
    "        print(f\"Creating dataset with {self.n_ts_keys} time series...\")\n",
    "        print(f\"Additional features: {self.n_features_additional}\")\n",
    "        print(f\"Sequence length: {seq_length}, Embargo: {embargo}\")\n",
    "        \n",
    "        # Group by time series key\n",
    "        grouped = df.sort_values('Date').groupby('ts_key')\n",
    "        \n",
    "        for ts_key, group in grouped:\n",
    "            values = group['Value'].values\n",
    "            dates = pd.to_datetime(group['Date'].values)\n",
    "            \n",
    "            # Extract additional features\n",
    "            if self.n_features_additional > 0:\n",
    "                additional_features = group[self.feature_cols].values\n",
    "            \n",
    "            # Skip if not enough data (need seq_length + embargo)\n",
    "            min_length = seq_length + embargo\n",
    "            if len(values) < min_length + 1:\n",
    "                continue\n",
    "            \n",
    "            # Get one-hot encoding for this ts_key\n",
    "            ts_key_idx = self.ts_key_to_idx[ts_key]\n",
    "            ts_key_onehot = np.zeros(self.n_ts_keys, dtype=np.float32)\n",
    "            ts_key_onehot[ts_key_idx] = 1.0\n",
    "            \n",
    "            # Create sliding windows with embargo\n",
    "            # If embargo=1: X ends at t-2, Y is at t (skipping t-1)\n",
    "            for i in range(len(values) - seq_length - embargo):\n",
    "                # Collect features for each timestep in the window\n",
    "                window_features = []\n",
    "                \n",
    "                for j in range(seq_length):\n",
    "                    # Features: [value, additional_features..., year, month, ts_key_onehot...]\n",
    "                    date = dates[i + j]\n",
    "                    \n",
    "                    features_list = [values[i + j]]  # Value\n",
    "                    \n",
    "                    # Add additional features if present\n",
    "                    if self.n_features_additional > 0:\n",
    "                        features_list.append(additional_features[i + j])\n",
    "                    \n",
    "                    # Add temporal features\n",
    "                    features_list.extend([date.year, date.month])\n",
    "                    \n",
    "                    # Add one-hot encoding\n",
    "                    features_list.append(ts_key_onehot)\n",
    "                    \n",
    "                    # Concatenate all features\n",
    "                    features = np.concatenate([\n",
    "                        np.array(f).flatten() if not isinstance(f, (int, float)) else [f]\n",
    "                        for f in features_list\n",
    "                    ])\n",
    "                    \n",
    "                    window_features.append(features)\n",
    "                \n",
    "                self.X.append(np.array(window_features))  # Shape: (seq_length, n_features)\n",
    "                # Target is embargo periods ahead from last observation\n",
    "                self.y.append(values[i + seq_length + embargo - 1])\n",
    "                self.ts_keys_list.append(ts_key)\n",
    "        \n",
    "        self.X = np.array(self.X, dtype=np.float32)  # Shape: (n_samples, seq_length, n_features)\n",
    "        self.y = np.array(self.y, dtype=np.float32)  # Shape: (n_samples,)\n",
    "        \n",
    "        print(f\"Created {len(self.X)} samples with feature dimension: {self.X.shape[2]}\")\n",
    "        print(f\"  - Value: 1\")\n",
    "        print(f\"  - Additional features: {self.n_features_additional}\")\n",
    "        print(f\"  - Temporal (year, month): 2\")\n",
    "        print(f\"  - One-hot ts_key: {self.n_ts_keys}\")\n",
    "        \n",
    "        # Train-test split (chronological)\n",
    "        n_samples = len(self.X)\n",
    "        train_size = int(n_samples * train_ratio)\n",
    "        \n",
    "        if train:\n",
    "            self.X = self.X[:train_size]\n",
    "            self.y = self.y[:train_size]\n",
    "            self.ts_keys_list = self.ts_keys_list[:train_size]\n",
    "        else:\n",
    "            self.X = self.X[train_size:]\n",
    "            self.y = self.y[train_size:]\n",
    "            self.ts_keys_list = self.ts_keys_list[train_size:]\n",
    "        \n",
    "        # Standardize features (Value + additional features + year + month - NOT one-hot)\n",
    "        self.scaler_X = StandardScaler()\n",
    "        self.scaler_y = StandardScaler()\n",
    "        \n",
    "        # Extract continuous features for scaling\n",
    "        n_samples, seq_len, n_features = self.X.shape\n",
    "        n_continuous = 1 + self.n_features_additional + 2  # Value + additional + year + month\n",
    "        \n",
    "        X_continuous = self.X[:, :, :n_continuous].reshape(-1, n_continuous)\n",
    "        X_onehot = self.X[:, :, n_continuous:].reshape(-1, self.n_ts_keys)\n",
    "        \n",
    "        if train:\n",
    "            X_continuous_scaled = self.scaler_X.fit_transform(X_continuous)\n",
    "            X_continuous_scaled = X_continuous_scaled.reshape(n_samples, seq_len, n_continuous)\n",
    "            X_onehot_reshaped = X_onehot.reshape(n_samples, seq_len, self.n_ts_keys)\n",
    "            \n",
    "            # Concatenate scaled continuous + unscaled one-hot\n",
    "            self.X = np.concatenate([X_continuous_scaled, X_onehot_reshaped], axis=2)\n",
    "            self.y = self.scaler_y.fit_transform(self.y.reshape(-1, 1)).flatten()\n",
    "        else:\n",
    "            # For test set, apply provided scalers or leave unscaled\n",
    "            if scaler_X is not None and scaler_y is not None:\n",
    "                self.scaler_X = scaler_X\n",
    "                self.scaler_y = scaler_y\n",
    "                \n",
    "                # Apply the training scalers to test set\n",
    "                X_continuous_scaled = self.scaler_X.transform(X_continuous)\n",
    "                X_continuous_scaled = X_continuous_scaled.reshape(n_samples, seq_len, n_continuous)\n",
    "                X_onehot_reshaped = X_onehot.reshape(n_samples, seq_len, self.n_ts_keys)\n",
    "                \n",
    "                # Concatenate scaled continuous + unscaled one-hot\n",
    "                self.X = np.concatenate([X_continuous_scaled, X_onehot_reshaped], axis=2)\n",
    "                self.y = self.scaler_y.transform(self.y.reshape(-1, 1)).flatten()\n",
    "            else:\n",
    "                # Scalers not provided - data remains unscaled (will need to be scaled externally)\n",
    "                pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.X[idx]), torch.FloatTensor([self.y[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now observe how to use this Dataset class in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "fold_config = FOLDS[2]\n",
    "\n",
    "full_path = os.path.join(cwd, \"..\", \"data\", \"gold\", \"monthly_registration_volume_gold.parquet\")\n",
    "output_path = os.path.join(cwd, \"models\", \"lstm\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "df_full = pd.read_parquet(full_path, engine='pyarrow')\n",
    "df_full['Year'] = df_full['Date'].dt.year\n",
    "df_full['Month'] = df_full['Date'].dt.month\n",
    "\n",
    "date_col = 'Date'\n",
    "ts_key_col = 'ts_key'\n",
    "value_col = 'Value'\n",
    "features = [col for col in df_full.columns if col not in [date_col, ts_key_col, value_col]]\n",
    "\n",
    "df_train = df_full[df_full['Date'] <= fold_config['train_end']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with 1502 time series...\n",
      "Additional features: 26\n",
      "Sequence length: 6, Embargo: 1\n",
      "Created 92931 samples with feature dimension: 1531\n",
      "  - Value: 1\n",
      "  - Additional features: 26\n",
      "  - Temporal (year, month): 2\n",
      "  - One-hot ts_key: 1502\n"
     ]
    }
   ],
   "source": [
    "SEQ_LENGTH = 6\n",
    "TRAIN_RATIO = 0.8\n",
    "EMBARGO = 1\n",
    "\n",
    "train_dataset = TimeSeriesDataset(\n",
    "            df_train,\n",
    "            feature_cols=features, \n",
    "            seq_length=SEQ_LENGTH,\n",
    "            embargo=EMBARGO,\n",
    "            train=True,\n",
    "            train_ratio=TRAIN_RATIO\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "  X Values: tensor([[-0.4596, -2.4260, -0.9106,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.5050, -2.3589,  0.1360,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4142, -2.2001,  0.5762,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4640, -2.0241,  0.6590,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4259, -1.9154,  0.3360,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4376, -1.8967, -0.0959,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "  y (scaled): -0.5079910159111023\n",
      "  y (original): 20.999996185302734\n",
      "Sample 1:\n",
      "  X Values: tensor([[-0.5050, -2.3589,  0.1360,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4142, -2.2001,  0.5762,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4640, -2.0241,  0.6590,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4259, -1.9154,  0.3360,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4376, -1.8967, -0.0959,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.5079, -1.9353, -0.3708,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "  y (scaled): -0.4782542288303375\n",
      "  y (original): 40.999996185302734\n",
      "Sample 2:\n",
      "  X Values: tensor([[-0.4142, -2.2001,  0.5762,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4640, -2.0241,  0.6590,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4259, -1.9154,  0.3360,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4376, -1.8967, -0.0959,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.5079, -1.9353, -0.3708,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4786, -1.5193,  1.8103,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "  y (scaled): -0.49460944533348083\n",
      "  y (original): 29.999996185302734\n"
     ]
    }
   ],
   "source": [
    "# Let's print the first values \n",
    "for i in range(3):\n",
    "    X_sample, y_sample = train_dataset[i]\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"  X Values: {X_sample}\")\n",
    "    print(f\"  y (scaled): {y_sample.item()}\")\n",
    "    # Inverse transform y to original scale\n",
    "    y_original = train_dataset.scaler_y.inverse_transform(y_sample.numpy().reshape(-1, 1)).flatten()[0]\n",
    "    print(f\"  y (original): {y_original}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can observe the first three samples from the training dataset, showing both the scaled and original target values. We see the many-to-one mapping from the input sequences to the target values, confirming that the dataset is structured correctly for our regression task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset \n",
    "\n",
    "For the testdataset we reused the scales created in the training dataset as well as the timeseries keys from the one-hot-encoding to map back our timeseries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with 1502 time series...\n",
      "Additional features: 26\n",
      "Sequence length: 6, Embargo: 1\n",
      "Created 92931 samples with feature dimension: 1531\n",
      "  - Value: 1\n",
      "  - Additional features: 26\n",
      "  - Temporal (year, month): 2\n",
      "  - One-hot ts_key: 1502\n"
     ]
    }
   ],
   "source": [
    "# Save scalers and metadata\n",
    "scaler_X = train_dataset.scaler_X\n",
    "scaler_y = train_dataset.scaler_y\n",
    "n_ts_keys = train_dataset.n_ts_keys\n",
    "ts_key_to_idx = train_dataset.ts_key_to_idx\n",
    "\n",
    "test_dataset = TimeSeriesDataset(\n",
    "    df_train,\n",
    "    feature_cols=features,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    train=False,\n",
    "    embargo=EMBARGO,\n",
    "    train_ratio=TRAIN_RATIO,\n",
    "    scaler_X=scaler_X,\n",
    "    scaler_y=scaler_y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sample 0:\n",
      "  X Values: tensor([[ 0.0074,  0.2044, -1.0369,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1448,  0.5645,  1.5417,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3044,  1.0385,  2.0883,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1214,  1.0305, -0.2239,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1653,  1.0359, -0.1598,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4083,  0.7582, -1.5177,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "  y (scaled): -0.4589253067970276\n",
      "  y (original): 53.999996185302734\n",
      "Test Sample 1:\n",
      "  X Values: tensor([[-0.1448,  0.5645,  1.5417,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3044,  1.0385,  2.0883,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1214,  1.0305, -0.2239,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1653,  1.0359, -0.1598,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4083,  0.7582, -1.5177,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4596,  0.5331, -1.2654,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "  y (scaled): -0.35187286138534546\n",
      "  y (original): 126.0\n",
      "Test Sample 2:\n",
      "  X Values: tensor([[-0.3044,  1.0385,  2.0883,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1214,  1.0305, -0.2239,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1653,  1.0359, -0.1598,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4083,  0.7582, -1.5177,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4596,  0.5331, -1.2654,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3542,  0.6070,  0.1690,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "  y (scaled): -0.2195441722869873\n",
      "  y (original): 215.0\n"
     ]
    }
   ],
   "source": [
    "# Let's print the first values from test set\n",
    "for i in range(3):\n",
    "    X_sample, y_sample = test_dataset[i]\n",
    "    print(f\"Test Sample {i}:\")\n",
    "    print(f\"  X Values: {X_sample}\")\n",
    "    print(f\"  y (scaled): {y_sample.item()}\")\n",
    "    # Inverse transform y to original scale\n",
    "    y_original = test_dataset.scaler_y.inverse_transform(y_sample.numpy().reshape(-1, 1)).flatten()[0]\n",
    "    print(f\"  y (original): {y_original}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the training and test datasets created with the specified sequence length and embargo period. The datasets include standardized features and targets, along with one-hot encoded time series keys. This can be now be used to train any model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training function implements a standard PyTorch training loop, as defined in the [Documentation](https://docs.pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) with the following steps:\n",
    "\n",
    "1. **Enable Training Mode**: Activates dropout and batch normalization layers for training\n",
    "2. **Batch Processing**: Iterates through batches from the data loader\n",
    "3. **Device Transfer**: Moves data to GPU (CUDA/MPS) if available for faster computation\n",
    "4. **Gradient Reset**: Clears gradients from the previous iteration to prevent accumulation\n",
    "5. **Forward Pass**: Feeds input data through the model to generate predictions\n",
    "6. **Loss Calculation**: Computes the difference between predictions and actual values using the specified criterion\n",
    "7. **Backward Pass**: Calculates gradients via backpropagation\n",
    "8. **Gradient Clipping**: Prevents exploding gradients by capping gradient norms at 1.0\n",
    "9. **Weight Update**: Applies the optimizer to update model parameters\n",
    "10. **Loss Tracking**: Accumulates batch losses to compute the average training loss\n",
    "\n",
    "The function returns the **average loss per batch**, providing a metric to monitor training progress across epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "\n",
    "    # Enable training mode\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for X_batch, y_batch in loader:\n",
    "        # Move data to MPS/CUDA if available\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        # Clear all gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Pass the batch through the model\n",
    "        predictions = model(X_batch)\n",
    "\n",
    "        loss = criterion(predictions, y_batch)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        # Prevetns exploding gradients, scales gradients if norm exceeds max_norm\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add batch loss to total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Divides by number of batches to get average loss per batch\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Function\n",
    "\n",
    "The validation function evaluates model performance on unseen data without updating weights. Key characteristics:\n",
    "\n",
    "1. **Evaluation Mode**: Disables dropout and freezes batch normalization for consistent predictions\n",
    "2. **No Gradient Computation**: Uses `torch.no_grad()` to reduce memory usage and speed up inference\n",
    "3. **Batch Processing**: Iterates through validation batches, computing predictions and loss\n",
    "4. **Device Transfer**: Ensures data is on the same device (CPU/GPU) as the model\n",
    "5. **Loss Aggregation**: Returns average loss across all validation batches\n",
    "\n",
    "This function provides an unbiased estimate of model performance, helping to detect overfitting during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, device):\n",
    "    # Enable evaluation mode (disables dropout and batch norm updates)\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    # Disable gradient calculation for evaluation (saves memory and computation)\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            # Move data to MPS/CUDA if available\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            # Pass the batch through the model\n",
    "            predictions = model(X_batch)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            \n",
    "            # Add batch loss to total loss\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    # Divides by number of batches to get average loss per batch\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Out-of-Sample Predictions\n",
    "\n",
    "This function generates out-of-sample forecasts using **batched autoregressive prediction**, which is an optimized approach for multi-step-ahead forecasting across multiple time series.\n",
    "\n",
    "### High-Level Process\n",
    "\n",
    "**1. Data Preparation**\n",
    "    - Extract the relevant time window (historical + test period) for all time series\n",
    "    - Initialize storage for each time series containing:\n",
    "      - Historical values and features (sequence length + embargo period)\n",
    "      - Test period dates and actual values\n",
    "      - One-hot encoding indices\n",
    "      - Empty prediction lists\n",
    "\n",
    "**2. Determine Prediction Horizon**\n",
    "    - Find the maximum number of predictions needed across all time series\n",
    "    - This defines how many forecasting steps the function will iterate through\n",
    "\n",
    "**3. Batched Autoregressive Forecasting Loop**\n",
    "    - For each prediction step (month):\n",
    "      - Collect all time series that need a prediction at this step\n",
    "      - Build input sequences for each time series using their current history\n",
    "      - Stack sequences into a single batch array\n",
    "      - Apply feature scaling (continuous features only, preserve one-hot encoding)\n",
    "      - Run one forward pass through the model for the entire batch\n",
    "      - Inverse-transform predictions back to original scale\n",
    "      - Clip negative predictions to zero\n",
    "\n",
    "**4. History Update**\n",
    "    - For each time series in the batch:\n",
    "      - Store the prediction\n",
    "      - Update history with actual values from the test set\n",
    "      - Append actual features for the next prediction step\n",
    "    - This creates a realistic forecasting scenario where predictions are made sequentially\n",
    "\n",
    "**5. Results Aggregation**\n",
    "    - Compile predictions and actuals into dictionaries (one per time series)\n",
    "    - Flatten all predictions and actuals into arrays for metric calculation\n",
    "    - Return both individual time series results and aggregated arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_out_of_sample_predictions(\n",
    "    model, \n",
    "    df_test_period,\n",
    "    df_full, \n",
    "    fold_config, \n",
    "    features, \n",
    "    scaler_X, \n",
    "    scaler_y, \n",
    "    ts_key_to_idx, \n",
    "    n_ts_keys, \n",
    "    seq_length, \n",
    "    embargo, \n",
    "    device\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate out-of-sample predictions using BATCHED autoregressive forecasting.\n",
    "    \n",
    "    This function optimizes the prediction process by:\n",
    "    1. Batches all time series together for each prediction step\n",
    "    2. Runs ONE forward pass for all time series simultaneously\n",
    "    3. Updates all histories together\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        df_test_period: Test period dataframe\n",
    "        df_full: Complete dataframe with all data\n",
    "        fold_config: Dict with 'test_start' and 'test_end' dates\n",
    "        features: List of feature column names\n",
    "        scaler_X: Fitted StandardScaler for features\n",
    "        scaler_y: Fitted StandardScaler for target\n",
    "        ts_key_to_idx: Dict mapping ts_key to one-hot index\n",
    "        n_ts_keys: Total number of time series\n",
    "        seq_length: Lookback window size\n",
    "        embargo: Gap between last observation and prediction\n",
    "        device: torch.device (cpu/cuda/mps)\n",
    "    \n",
    "    Returns:\n",
    "        predictions_dict: Dict mapping ts_key -> list of predictions\n",
    "        actuals_dict: Dict mapping ts_key -> list of actual values\n",
    "        all_preds: Flattened array of all predictions\n",
    "        all_acts: Flattened array of all actuals\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"STEP 3: Out-of-sample predictions (OPTIMIZED)\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    print(f\"Test period: {fold_config['test_start']} to {fold_config['test_end']}\")\n",
    "    print(f\"Test observations: {len(df_test_period):,}\")\n",
    "    \n",
    "    # Setup\n",
    "    test_start_date = pd.to_datetime(fold_config['test_start'])\n",
    "    lookback_start = test_start_date - pd.DateOffset(months=seq_length + embargo)\n",
    "    \n",
    "    df_for_prediction = df_full[\n",
    "        (df_full['Date'] >= lookback_start) & \n",
    "        (df_full['Date'] <= fold_config['test_end'])\n",
    "    ].copy()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 1: Initialize data structures for ALL time series at once\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    ts_data = {}  # Master dictionary storing everything about each time series\n",
    "    valid_ts_keys = []  # Time series with enough history\n",
    "    \n",
    "    for ts_key, group in df_for_prediction.groupby('ts_key'):\n",
    "        group = group.sort_values('Date')\n",
    "        \n",
    "        # Skip if not in training set\n",
    "        if ts_key not in ts_key_to_idx:\n",
    "            continue\n",
    "        \n",
    "        # Get historical data\n",
    "        hist_data = group[group['Date'] < test_start_date]\n",
    "        \n",
    "        if len(hist_data) < seq_length + embargo:\n",
    "            continue\n",
    "        \n",
    "        # Get test data\n",
    "        test_data = group[group['Date'] >= test_start_date]\n",
    "        \n",
    "        if len(test_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Initialize time series data\n",
    "        ts_data[ts_key] = {\n",
    "            'ts_key_idx': ts_key_to_idx[ts_key],\n",
    "            'recent_values': hist_data['Value'].values[-(seq_length + embargo):].copy(),\n",
    "            'recent_features': hist_data[features].values[-(seq_length + embargo):].copy(),\n",
    "            'recent_dates': pd.to_datetime(hist_data['Date'].values[-(seq_length + embargo):]),\n",
    "            'test_dates': test_data['Date'].values,\n",
    "            'test_actuals': test_data['Value'].values,\n",
    "            'predictions': [],\n",
    "            'n_predictions': len(test_data)\n",
    "        }\n",
    "        valid_ts_keys.append(ts_key)\n",
    "    \n",
    "    if len(valid_ts_keys) == 0:\n",
    "        print(\"No valid time series found!\")\n",
    "        return {}, {}, np.array([]), np.array([])\n",
    "    \n",
    "    print(f\"Processing {len(valid_ts_keys)} time series in batched mode...\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 2: Determine maximum prediction horizon across all time series\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    max_horizon = max(ts_data[ts_key]['n_predictions'] for ts_key in valid_ts_keys)\n",
    "    n_continuous = 1 + len(features) + 2\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 3: Autoregressive prediction - ONE BATCH PER TIME STEP\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    for step in range(max_horizon):\n",
    "        # Collect all time series that need prediction at this step\n",
    "        batch_ts_keys = []\n",
    "        batch_sequences = []\n",
    "        \n",
    "        for ts_key in valid_ts_keys:\n",
    "            data = ts_data[ts_key]\n",
    "            \n",
    "            # Skip if this time series has already made all its predictions\n",
    "            if step >= data['n_predictions']:\n",
    "                continue\n",
    "            \n",
    "            # Build sequence for this time series\n",
    "            sequence = []\n",
    "            ts_key_onehot = np.zeros(n_ts_keys, dtype=np.float32)\n",
    "            ts_key_onehot[data['ts_key_idx']] = 1.0\n",
    "            \n",
    "            for i in range(seq_length):\n",
    "                idx = len(data['recent_values']) - seq_length - embargo + i\n",
    "                \n",
    "                if idx < 0 or idx >= len(data['recent_values']):\n",
    "                    break\n",
    "                \n",
    "                date = pd.to_datetime(data['recent_dates'][idx])\n",
    "                value = data['recent_values'][idx]\n",
    "                feat = data['recent_features'][idx]\n",
    "                \n",
    "                # Build feature vector\n",
    "                features_list = [value]\n",
    "                features_list.append(feat)\n",
    "                features_list.extend([date.year, date.month])\n",
    "                features_list.append(ts_key_onehot)\n",
    "                \n",
    "                feature_vector = np.concatenate([\n",
    "                    np.array(f).flatten() if not isinstance(f, (int, float)) else [f]\n",
    "                    for f in features_list\n",
    "                ])\n",
    "                \n",
    "                sequence.append(feature_vector)\n",
    "            \n",
    "            # Only add if we have a complete sequence\n",
    "            if len(sequence) == seq_length:\n",
    "                batch_ts_keys.append(ts_key)\n",
    "                batch_sequences.append(sequence)\n",
    "        \n",
    "        # If no time series to predict at this step, continue\n",
    "        if len(batch_sequences) == 0:\n",
    "            continue\n",
    "        \n",
    "        # -------------------------------------------------------------------------\n",
    "        # STEP 4: Batch prediction for all time series at this step\n",
    "        # -------------------------------------------------------------------------\n",
    "        \n",
    "        # Stack all sequences into a batch\n",
    "        batch_array = np.array(batch_sequences, dtype=np.float32)  # (batch_size, seq_length, n_features)\n",
    "        \n",
    "        # Separate continuous and one-hot features\n",
    "        batch_continuous = batch_array[:, :, :n_continuous]  # (batch_size, seq_length, n_continuous)\n",
    "        batch_onehot = batch_array[:, :, n_continuous:]      # (batch_size, seq_length, n_onehot)\n",
    "        \n",
    "        # Scale continuous features\n",
    "        batch_size, seq_len, n_cont = batch_continuous.shape\n",
    "        batch_continuous_flat = batch_continuous.reshape(-1, n_cont)\n",
    "        batch_continuous_scaled = scaler_X.transform(batch_continuous_flat)\n",
    "        batch_continuous_scaled = batch_continuous_scaled.reshape(batch_size, seq_len, n_cont)\n",
    "        \n",
    "        # Recombine\n",
    "        batch_scaled = np.concatenate([batch_continuous_scaled, batch_onehot], axis=2)\n",
    "        \n",
    "        # Single forward pass for entire batch\n",
    "        with torch.no_grad():\n",
    "            X_batch = torch.FloatTensor(batch_scaled).to(device)\n",
    "            pred_scaled_batch = model(X_batch).cpu().numpy()  # (batch_size, 1)\n",
    "            pred_values_batch = scaler_y.inverse_transform(pred_scaled_batch).flatten()\n",
    "        \n",
    "        # All negative predictions are set to zero\n",
    "        pred_values_batch = np.maximum(pred_values_batch, 0.0)\n",
    "\n",
    "        # -------------------------------------------------------------------------\n",
    "        # STEP 5: Update histories for all time series in batch\n",
    "        # -------------------------------------------------------------------------\n",
    "        \n",
    "        for i, ts_key in enumerate(batch_ts_keys):\n",
    "            data = ts_data[ts_key]\n",
    "            pred_value = pred_values_batch[i]\n",
    "            \n",
    "            # Store prediction\n",
    "            data['predictions'].append(pred_value)\n",
    "            \n",
    "            # Get actual value and features for this prediction\n",
    "            pred_date = pd.to_datetime(data['test_dates'][step])\n",
    "            \n",
    "            # Use actual value from test set\n",
    "            actual_value = data['test_actuals'][step]\n",
    "            \n",
    "            # Get actual features or carry forward\n",
    "            test_group = df_for_prediction[\n",
    "                (df_for_prediction['ts_key'] == ts_key) & \n",
    "                (df_for_prediction['Date'] == pred_date)\n",
    "            ]\n",
    "            \n",
    "            if len(test_group) > 0:\n",
    "                actual_features = test_group[features].values[0]\n",
    "            else:\n",
    "                actual_features = data['recent_features'][-1]\n",
    "            \n",
    "            # Update history\n",
    "            data['recent_values'] = np.append(data['recent_values'], actual_value)\n",
    "            data['recent_features'] = np.vstack([data['recent_features'], actual_features])\n",
    "            data['recent_dates'] = np.append(data['recent_dates'], pred_date)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 6: Collect results\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    predictions_dict = {}\n",
    "    actuals_dict = {}\n",
    "    \n",
    "    for ts_key in valid_ts_keys:\n",
    "        data = ts_data[ts_key]\n",
    "        if len(data['predictions']) > 0:\n",
    "            predictions_dict[ts_key] = data['predictions']\n",
    "            actuals_dict[ts_key] = data['test_actuals'][:len(data['predictions'])]\n",
    "    \n",
    "    all_preds = np.concatenate([np.array(v) for v in predictions_dict.values()])\n",
    "    all_acts = np.concatenate([np.array(v) for v in actuals_dict.values()])\n",
    "    \n",
    "    print(f\"Generated predictions for {len(predictions_dict)} time series\")\n",
    "    print(f\"Total predictions: {len(all_preds):,}\")\n",
    "    print(f\"Optimization: {len(valid_ts_keys) * max_horizon} individual predictions → {max_horizon} batched forward passes\")\n",
    "    \n",
    "    return predictions_dict, actuals_dict, all_preds, all_acts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
